{"cells":[{"cell_type":"markdown","source":["### Objective:\nThe objective of the notebook is to -\n* Build the final model of LSTM algorithm using the best hyperparameter set (identified using Backtesting) and score the test set to get a performance metric\n* For forecasting future periods, we will re-train the model with the same hyperparameter set on the train + validation + test set to capture the patterns in the test set and then forecast future N periods"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3244205d-09ab-4dd6-9930-3abc03cd8091","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import yaml\nimport inspect\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom distutils.command.config import config\nfrom tqdm.auto import tqdm\nfrom datetime import timedelta\nfrom datetime import datetime\nimport mlflow\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nimport os\nimport logging\nimport dotsi\nimport tensorflow\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM,TimeDistributed,RepeatVector"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8afbc8bf-179f-4ffb-893e-37b23843914e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# logging part\np_dir = \"/tmp/\"\nlog_file = \"LSTM_model_eval_retraining_scoring\" + \" (\" +datetime.today().strftime('%Y-%m-%d-%H-%M-%S')+ \").log\"\n\nlogger = logging.getLogger('custom_log')\nlogger.setLevel(logging.DEBUG)\n\n# Applying necessary formatter\nfh = logging.FileHandler(p_dir+log_file)\nformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\nfh.setFormatter(formatter)\nlogger.addHandler(fh)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6894dc1d-f97c-45c7-a5dd-3ec4ee2964db","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Getting the default settings of hyperparameters. Used to check that user-provided hyperparameters must always be a subset of these.\ndef get_default_args(func) -> dict:\n    \"\"\"Function to get the default values of the hyperparameters for the given algorithm\n\n    Parameters\n    ----------\n    func : constructor of the respective algorithm\n        The name of the algorithm (Eg: Prophet,SARIMAX)\n\n    Returns\n    -------\n    dict\n        returns a dictionary of hyperparameters and the corresponding default values for the given algorithm\n    \"\"\"\n    signature = inspect.signature(func)\n    return {\n        k: v.default if v.default is not inspect.Parameter.empty else None\n        for k, v in signature.parameters.items()\n        if k != 'self'\n    }\n    \ndefault_hpps_compile = get_default_args(Sequential.compile)\ndefault_hpps_fit = get_default_args(Sequential.fit)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"497f0fb9-2e94-40d4-9fea-50103670624e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%run ../../../0_Config.ipynb"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b3b174e4-abbc-45b5-8235-7376db53681a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["logger.info(\"Config file read\")\nassert set(app_config[\"Algorithms\"][\"LSTM\"][\"Hyperparameters\"]['compile'].keys()).\\\n           issubset(set(default_hpps_compile.keys())),\\\n           'keys supplied by the user for the LSTM Algorithm under comiple method must be valid'\nassert set(app_config[\"Algorithms\"][\"LSTM\"][\"Hyperparameters\"]['fit'].keys()).\\\n           issubset(set(default_hpps_fit.keys())),\\\n           'keys supplied by the user for the LSTM Algorithm under fit method must be valid'\n\n# For exporting the config file\ntemp_config = app_config.copy()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4a08f9d9-4c5c-4e70-a595-bee153da17f8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def frange(start,stop,step= 1):\n    l = []\n    i = start\n    while(i < stop):\n        l.append(round(i,len(str(step))))\n        i = i+step\n    return l\n\ndef drange(hyperparameters):\n    l=[]\n    for key in hyperparameters.keys():\n        val = hyperparameters[key]\n        if 'range' in val:\n            val = val.replace('range','frange')\n            new_str = 'total_list = '  + val\n            _locals = locals()\n            exec(new_str,globals(),_locals)\n            without_dup = list(set(_locals['total_list']))\n            hyperparameters[key] = without_dup\n    return hyperparameters"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f5c9513a-f351-4b2e-920b-ea5f0ebb7258","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["fit_ = drange(app_config['Algorithms']['LSTM']['Hyperparameters']['fit'])\ncompile_ = drange(app_config['Algorithms']['LSTM']['Hyperparameters']['compile'])\nfor key in compile_.keys():\n    if(key in fit_.keys()):\n        fit_[key] = list(set(fit_[key]+compile_[key]))\n    else:\n        fit_[key] = list(compile_[key])\n    \nfit_new = {}\nfor key in fit_.keys():\n    temp = []\n    for val in fit_[key]:\n        if(type(val) == list):\n            val = str(val)\n        if((val!='None') and (val!='Null') and (val!=None)):\n            temp.append(val)\n    if(len(temp)>0):\n        fit_new[key] = temp\n        \nif('kwargs' in fit_new.keys()):\n    del fit_new['kwargs']\n    \nfor val in ['x','y','validation_data','kwargs']:\n    if(val in fit_new.keys()):\n        del fit_new[val]\n        \napp_config[\"Algorithms\"][\"LSTM\"][\"Hyperparameters\"] = fit_new"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0fadf3ea-771b-48cb-a5d2-835fa2838158","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Create the algo directory for storing the results\noutput_directory = app_config['output_dir_path']\nroot_dir = \"Modeling_Results\"\nalgorithm = \"LSTM\"\nalgo_path = os.path.join(output_directory,root_dir,algorithm)\nif not os.path.exists(algo_path):\n    os.makedirs(algo_path)\nlogger.info(\"Created algorithm directory\")    \n\nlogs_path = os.path.join(output_directory,root_dir,'logs',algorithm)\nif not os.path.exists(logs_path):\n    os.makedirs(logs_path)\nlogger.info(\"Created logs directory\")\n\nconfig_path = os.path.join(app_config['output_dir_path'],\"Modeling_Results\",\"config\")\nif not os.path.exists(config_path):\n    os.makedirs(config_path)\nlogger.info(\"Created config directory\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bdb403d4-f130-4e55-86d6-c0717353e44f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["hyperparameters_conf = dict(app_config[\"Algorithms\"][\"LSTM\"][\"Hyperparameters\"])\n# print(hyperparameters_conf)\n\nmodeling_granularity_conf = app_config[\"modeling_granularity\"]\n# print(modeling_granularity_conf)\n\n# Rename Start date and DV config\ndv_config = app_config[\"dependent_variable\"]\nds_config = app_config[\"date_var\"]\n\n# pos and neg corr broadcast\ncorr_config = dict(app_config['Algorithms']['LSTM']['exogenous_variables'])\ncorr_config_broadcast = dotsi.Dict({\"value\":corr_config})\n\n# Eval metric broadcast\nbroadcast_metric = dotsi.Dict({\"value\":app_config['validation']['metric']})\nbroadcast_test_periods = dotsi.Dict({\"value\":app_config[\"validation\"][\"no_of_test_periods\"]})\n\nbroadcast_regressors = dotsi.Dict({\"value\":list(set(corr_config['positive_corr']+corr_config['negative_corr']+corr_config['uncertain_corr']))})\nbroadcast_granularity = dotsi.Dict({\"value\":modeling_granularity_conf})\nbroadcast_hyper_parameters = dotsi.Dict({\"value\":hyperparameters_conf})\nbroadcast_forecast_periods = dotsi.Dict({\"value\":app_config[\"Algorithms\"][\"LSTM\"][\"forecast_periods\"]})\nbroadcast_lookback_periods = dotsi.Dict({\"value\":app_config[\"Algorithms\"][\"LSTM\"][\"lookback_periods\"]})\nbroadcast_tracking = dotsi.Dict({\"value\":app_config['tracking']})\nmlflow_tracking_check = dotsi.Dict({\"value\":\"Out of Sample\"})\nlogger.info(\"Broadcasted the required variables\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a06d2154-c076-49d8-bb35-d7a3809fd115","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Reading the latest file based on timestamp\nall_files = [file for file in os.listdir(algo_path)]\nbest_hyp_files = [file for file in all_files if \"Best_hyperparameters (\" in file]\nbest_hyp_files = [file.replace(\".csv\",\"\") for file in best_hyp_files]\nversion_dates = [datetime.strptime(x.split('(')[1].replace(')',''), '%Y-%m-%d-%H-%M-%S') for x in best_hyp_files]\nmax_date = max(version_dates)\nmax_date = max_date.strftime('%Y-%m-%d-%H-%M-%S')\nreq_file_name = [x for x in best_hyp_files if max_date in x]\nbest_hyp_param_results_file_path = os.path.join(algo_path,req_file_name[0]+\".csv\")\nprint(best_hyp_param_results_file_path)\n\nbest_hyperparam_results = pd.read_csv(best_hyp_param_results_file_path)\nbest_hyperparam_results = best_hyperparam_results[best_hyperparam_results['status']=='success'].reset_index(drop = True)\nbest_hyperparam_results[modeling_granularity_conf] = best_hyperparam_results[modeling_granularity_conf].astype(str)\nbest_hyperparam_results.replace(['true'],True, inplace = True)\nbest_hyperparam_results.replace(['false'],False, inplace = True)\nbest_hyperparam_results_broadcast = dotsi.Dict({\"value\":best_hyperparam_results})\nlogger.info(\"Read the best hyperparamter results\")\nbest_hyperparam_results"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b0058222-f420-49c7-a6ed-6593783f8270","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/dbfs/mnt/solutionsadls_data/modeling_results_python/Modeling_Results/LSTM/Best_hyperparameters (2023-01-13-10-33-13).csv\nOut[33]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/dbfs/mnt/solutionsadls_data/modeling_results_python/Modeling_Results/LSTM/Best_hyperparameters (2023-01-13-10-33-13).csv\nOut[33]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Div_No</th>\n      <th>Store_No</th>\n      <th>Base_UPC</th>\n      <th>epochs</th>\n      <th>batch_size</th>\n      <th>verbose</th>\n      <th>shuffle</th>\n      <th>loss</th>\n      <th>optimizer</th>\n      <th>mape</th>\n      <th>wmape</th>\n      <th>bias</th>\n      <th>tracking_signal</th>\n      <th>mae</th>\n      <th>rmse</th>\n      <th>status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24</td>\n      <td>15</td>\n      <td>4000046410</td>\n      <td>50</td>\n      <td>32</td>\n      <td>2</td>\n      <td>False</td>\n      <td>mae</td>\n      <td>adam</td>\n      <td>107.415744</td>\n      <td>61.084682</td>\n      <td>-0.071489</td>\n      <td>-0.463355</td>\n      <td>4.145606</td>\n      <td>4.828644</td>\n      <td>success</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Div_No</th>\n      <th>Store_No</th>\n      <th>Base_UPC</th>\n      <th>epochs</th>\n      <th>batch_size</th>\n      <th>verbose</th>\n      <th>shuffle</th>\n      <th>loss</th>\n      <th>optimizer</th>\n      <th>mape</th>\n      <th>wmape</th>\n      <th>bias</th>\n      <th>tracking_signal</th>\n      <th>mae</th>\n      <th>rmse</th>\n      <th>status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24</td>\n      <td>15</td>\n      <td>4000046410</td>\n      <td>50</td>\n      <td>32</td>\n      <td>2</td>\n      <td>False</td>\n      <td>mae</td>\n      <td>adam</td>\n      <td>107.415744</td>\n      <td>61.084682</td>\n      <td>-0.071489</td>\n      <td>-0.463355</td>\n      <td>4.145606</td>\n      <td>4.828644</td>\n      <td>success</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Reading feature selected output and using the significant variables as idvs in modeling\nfeature_selection_info = app_config['Algorithms']['LSTM']['feature_selection']\nbroadcast_use_features = dotsi.Dict({\"value\":feature_selection_info['use_feature_selected_idvs']})\nif(feature_selection_info['use_feature_selected_idvs']):\n    if(feature_selection_info['approach']=='lasso_cvglmnet'):\n        output_folder = app_config['output_dir_path']+\"/Feature_Selection/Lasso/\"\n    # Reading the latest input file based on timestamp\n    coeff_op_files = [file for file in os.listdir(output_folder)]\n    coeff_op_files = [file.replace(\".csv\",\"\") for file in coeff_op_files]\n    version_dates = [datetime.strptime(x.split('(')[1].replace(')',''), '%Y-%m-%d-%H-%M-%S') for x in coeff_op_files]\n    max_date = max(version_dates)\n    max_date = max_date.strftime('%Y-%m-%d-%H-%M-%S')\n    req_file_name = [x for x in coeff_op_files if max_date in x]\n    coeff_op_file_path = os.path.join(output_folder,req_file_name[0] + \".csv\")\n    print(coeff_op_file_path)\n\n    # Reading the data\n    coeff_df = pd.read_csv(coeff_op_file_path)\n    coeff_df = coeff_df[coeff_df['status']=='success']\n    # print(coeff_df.shape)\n    coeff_df[modeling_granularity_conf] = coeff_df[modeling_granularity_conf].astype(str)\n    idvs_len = len(feature_selection_info['must_have_idvs'])\n    if(idvs_len>0):\n        temp1 = coeff_df[modeling_granularity_conf].drop_duplicates()\n        temp1['temp'] = 1\n        temp2 = pd.DataFrame({'IDV':feature_selection_info['must_have_idvs']})\n        temp2['temp'] = 1\n        temp = temp1.join(temp2, on = 'temp', how ='left')\n        req_cols = modeling_granularity_conf + ['IDV']\n        coeff_df = coeff_df.drop_duplicates()\n    coeffs_broadcast = dotsi.Dict({\"value\":coeff_df})\n    broadcast_regressors = dotsi.Dict({\"value\":list(coeff_df['IDV'].unique())})\n# display(coeff_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9da880d4-95a1-4273-b5a9-0fc5146ffad5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/dbfs/mnt/solutionsadls_data/modeling_results_python/Feature_Selection/Lasso/lasso_feature_selection_results (2023-01-13-04-31-21).csv\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/dbfs/mnt/solutionsadls_data/modeling_results_python/Feature_Selection/Lasso/lasso_feature_selection_results (2023-01-13-04-31-21).csv\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_forecast_UDF(df_data: pd.DataFrame)-> pd.DataFrame:\n    \"\"\"Function to perform final model building using the train data and score on the test data utilizing the broadcasted details from the config file\n\n    Parameters\n    ----------\n    df_data : pd.DataFrame\n        The dataset containing values for all the required variables\n\n    Returns\n    -------\n    pd.DataFrame\n        Returns a dataframe with the granularity,date,independent variables contributions if any and performance metrics for the training and the testing set\n    \"\"\"\n    try:\n        df_data = df_data.sort_values(by=['ds'],ascending=True)\n        hpt = best_hyperparam_results_broadcast.value\n        \n        # broadcast_granularity\n        broadcast_gran = broadcast_granularity.value\n        \n        # get best hyperparameters for the given modeling granularity\n        for x in list(broadcast_gran):\n            hpt = hpt[hpt[x] == df_data[x].iloc[0]]\n            \n        # number of test periods to look\n        lookback_period = broadcast_lookback_periods.value\n        forecast_period = broadcast_forecast_periods.value\n        test_periods1 = int(broadcast_test_periods.value)\n        test_periods = test_periods1 - forecast_period + 1 \n        \n        if(broadcast_use_features.value==True):\n            # Reading regressors from feature selection\n            coeffs_df = coeffs_broadcast.value\n            for x in broadcast_gran:\n                coeffs_df = coeffs_df[coeffs_df[x] == df_data[x].iloc[0]]\n            regressors = list(coeffs_df['IDV'].values)\n        else:\n            # Appending regressors based on the sign of correlation\n            corr_var = corr_config_broadcast.value\n            regressors = list(set(corr_var[\"positive_corr\"] + corr_var[\"negative_corr\"]+corr_var['uncertain_corr']))\n\n            temp_list1 = []\n            # Removing regressors based on the correlation\n            if(corr_var[\"consider_correlation\"]):   \n                for x in corr_var[\"positive_corr\"]:\n                    if(df_data[['y',x]].corr().iloc[0][1]<0):\n                        temp_list1.append(x)\n                for x in corr_var[\"negative_corr\"]:\n                    if (x not in temp_list1):\n                        if(df_data[['y',x]].corr().iloc[0][1]>0):\n                            temp_list1.append(x)   \n                regressors = list(set(regressors) - set(temp_list1))\n\n            # Checking for variance in the regressor\n            temp_list2 = []\n            if len(regressors)>0:\n                for ex_var in regressors:  \n                    mean = df_data[ex_var].mean()\n                    std = df_data[ex_var].std()\n                    if mean == 0:\n                        if std <= 0.001:\n                            temp_list2.append(ex_var)\n                    else:\n                        if abs(std/mean) <= 0.01:\n                            temp_list2.append(ex_var)\n\n            regressors = list(set(regressors) - set(temp_list2))\n            temp_list = temp_list1 + temp_list2\n        \n        # filtering for the required data\n        data = df_data[['y']+regressors].astype('float32')\n        values = data.values\n        n_vars = len(regressors) + 1\n        cols, names = list(), list()\n\n        # input sequence (t-n, ... t-1)\n        for i in range(lookback_period, 0, -1):\n            cols.append(data.shift(i))\n            names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n\n        # forecast sequence (t, t+1, ... t+n)\n        for i in range(0, forecast_period):\n            cols.append(data.shift(-i))\n            if i == 0:\n                names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n            else:\n                names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n\n        # put it all together\n        agg = pd.concat(cols, axis=1)\n        agg.columns = names\n\n        # drop rows with NaN values\n        if True:\n            agg.dropna(inplace=True)\n\n        # reframed = agg.copy()\n        cols_to_drop = [col for col in agg.columns if (\"var1\" not in col) & ((\"(t)\" in col) | (\"(t+\" in col))]\n        agg.drop(cols_to_drop, axis=1, inplace=True)\n\n        values = agg.values\n        train_val = values[:values.shape[0]-test_periods, :]\n        test_val = values[values.shape[0]-test_periods:, :]\n\n        # split into input and outputs\n        train_X, train_y = train_val[:, :-forecast_period], train_val[:, -forecast_period:]\n        test_X, test_y = test_val[:, :-forecast_period], test_val[:, -forecast_period:]\n\n        # reshape input to be 3D [samples, timesteps, features]\n        train_X = train_X.reshape((train_X.shape[0], lookback_period, n_vars))\n        test_X = test_X.reshape((test_X.shape[0], lookback_period, n_vars))\n\n        # Updating the default arguments with the parameters provided in the config\n        hp_config = broadcast_hyper_parameters.value\n        def_args = get_default_args(Sequential.compile)\n        for x in hp_config:\n            if(x in def_args.keys()):\n                temp_val = hpt[x].iloc[0]\n                if(type(temp_val)==str):\n                    if('[' in temp_val):\n                        temp_val = eval(temp_val)\n                def_args[x] = temp_val\n        if('kwargs' in def_args.keys()):\n            del def_args['kwargs']\n            \n        def_args_fit = get_default_args(Sequential.fit)\n        for x in hp_config:\n            if(x in def_args_fit.keys()):\n                temp_val = hpt[x].iloc[0]\n                if(type(temp_val)==str):\n                    if('[' in temp_val):\n                        temp_val = eval(temp_val)\n                def_args_fit[x] = temp_val\n        def_args_fit['x'] = train_X\n        def_args_fit['y'] = train_y\n        def_args_fit['validation_data'] = (test_X, test_y)\n        \n        # Calling the LSTM constructor with the hyperparameters of interest  \n        tensorflow.keras.utils.set_random_seed(1)\n        # design network\n        model = Sequential()\n        model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2]))) ## encoder \n        model.add(RepeatVector(forecast_period))\n        model.add(LSTM(50, activation='relu', return_sequences=True)) ## decoder\n        model.add(Dense(1))\n        model.compile(**def_args)\n        model.fit(**def_args_fit)\n        \n        ### predictions\n        # train_predict=model.predict(train_X)\n        test_predict=model.predict(test_X)\n        \n        # test period dates\n        test_dates = df_data.iloc[-test_periods1:]['ds'].values\n        test = pd.DataFrame()\n        for window in range(len(test_predict)):\n            test = pd.concat([test,pd.DataFrame({'ds':test_dates[window:forecast_period+window],\\\n                                                 'yhat':list(list(zip(*test_predict[window]))[0]),\\\n                                                 'window':np.repeat(window+1,forecast_period)})])\n        test['test_flag'] = 1\n        results_pd = pd.merge(test,df_data,how='left')\n        results_pd = results_pd[broadcast_gran+['ds', 'y', 'yhat','test_flag','window']].reset_index(drop = True)\n        # Sales or Quantity can't be negative hence\n        results_pd[\"yhat\"] = np.where(results_pd[\"yhat\"]<0,0,results_pd[\"yhat\"])\n        \n        # Eval. metrics calculation\n        # to handle erroneous results epsilon is set to 1.\n        epsilon = 1\n        temp_data1 = pd.DataFrame(index= range(1))\n        temp_data2 = pd.DataFrame()\n        for window in results_pd['window'].unique():\n            temp_data = results_pd[results_pd['window']==window]\n            y_pred = temp_data['yhat']\n            y_true = temp_data['y']\n\n            temp_data1['window'] = window\n            # Eval. metrics calculation\n            temp_data1['mape'] = np.mean(np.abs(y_true - y_pred) / np.maximum(np.abs(y_true), epsilon))*100  \n            temp_data1['wmape'] = np.sum(np.abs(y_true - y_pred)) / np.maximum(np.sum(np.abs(y_true)),epsilon)*100  \n            temp_data1['bias'] = np.mean((y_true - y_pred))  \n            temp_data1['tracking_signal'] = np.sum((y_true - y_pred)) / np.mean(np.abs(y_true - y_pred))\n            temp_data1['mae'] = mean_absolute_error(y_true, y_pred)\n            temp_data1['rmse']=np.sqrt(mean_squared_error(y_true, y_pred))\n            temp_data2 = pd.concat([temp_data2,temp_data1],ignore_index = True)\n            \n        results_pd = pd.merge(results_pd,temp_data2,how='left',on='window')\n        \n        # To adhere to defined schema\n        for x in broadcast_gran + ['window']:   \n            results_pd[x] = results_pd[x].astype(str)\n\n        # Append Hyperparameters used\n        for x in hp_config:\n            results_pd[x] = hpt[x].iloc[0]\n        \n        # Get the experiment id\n        tracking_value = broadcast_tracking.value.copy()\n        if(mlflow_tracking_check.value == \"Out of Sample\" and tracking_value[\"tracking_needed\"] == True):\n            if(tracking_value['type']!=\"Managed\"):\n                if(tracking_value['tracking_uri'] is not None):\n                    mlflow.set_tracking_uri(\"file:\"+tracking_value['tracking_uri'])\n                    experiment_id = mlflow.set_experiment(tracking_value[\"mlflow_experiment_id\"])\n                    tracking_value['mlflow_experiment_id'] = experiment_id.experiment_id\n            #Add MLFlow code here\n            with mlflow.start_run(experiment_id = tracking_value['mlflow_experiment_id']):\n                mlflow.log_param('algorithm', 'LSTM')\n                mlflow.log_param('result_type', 'out_of_sample')\n                for x in broadcast_gran:\n                    mlflow.log_param(x, results_pd[x].iloc[0])\n                for x in hp_config:\n                    mlflow.log_param(x, results_pd[x].iloc[0])\n                temp_test = results_pd[results_pd['test_flag']==1].reset_index(drop = True)\n                for x in [\"mape\",\"wmape\",\"bias\",\"tracking_signal\",\"mae\",\"rmse\"]:\n                    mlflow.log_metric(x, temp_test[x].iloc[0])\n                \n        results_pd['status'] = 'success'\n        return results_pd\n    \n    except Exception as e:\n        results_pd = pd.DataFrame(columns = [['ds', 'y', 'yhat','mape','wmape','bias','tracking_signal','mae','rmse']+\\\n                                              list(broadcast_hyper_parameters.value.keys()) + ['status','test_flag','window'] + broadcast_granularity.value],index = range(1))\n        results_pd[broadcast_granularity.value] = df_data[broadcast_granularity.value].head(1).reset_index(drop = True)\n        for x in broadcast_granularity.value:\n            results_pd[x] = results_pd[x].astype(str)\n        results_pd['status'] = str(e)  \n        return results_pd"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"82381fab-fab2-426e-836b-9ee459b21544","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Loading the latest Missing_value_treatment file\n##### Please update the reading path with the required data path if \"Missing value treatment\" was not run"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a3e2fc8f-d3ab-428c-a4a9-70f2e9b44549","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Reading the latest input file based on timestamp\nall_files = [file for file in os.listdir(app_config['output_dir_path']+\"/Data_Processing/Missing_value_treatment\")]\nmissing_op_files = [file for file in all_files if \"Missing_value_treatment_results (\" in file]\nmissing_op_files = [file.replace(\".csv\",\"\") for file in missing_op_files]\nversion_dates = [datetime.strptime(x.split('(')[1].replace(')',''), '%Y-%m-%d-%H-%M-%S') for x in missing_op_files]\nmax_date = max(version_dates)\nmax_date = max_date.strftime('%Y-%m-%d-%H-%M-%S')\nreq_file_name = [x for x in missing_op_files if max_date in x]\nmissing_op_file_path = os.path.join(app_config['output_dir_path']+\"/Data_Processing/Missing_value_treatment\",req_file_name[0]+'.csv')\n# print(missing_op_file_path)\n\n# Reading the data\ndf = pd.read_csv(missing_op_file_path)\n# print(df.shape)\n\ndf.rename(columns = {ds_config:\"ds\", dv_config:\"y\"}, inplace = True)\ndf['ds'] = pd.to_datetime(df['ds'])\n\ndf[modeling_granularity_conf] = df[modeling_granularity_conf].astype(str)\n\nlogger.info(\"Data loaded\")\n# print(list(broadcast_hyper_parameters.value.keys()))\n\ngbcp = list(modeling_granularity_conf)\n\ndf['gran_tempp'] = df[gbcp].astype(str).sum(axis=1)\nunique_pdts = df['gran_tempp'].unique()\ndf_forecast = pd.DataFrame()\nfor pdt in unique_pdts:\n    df_forecast = pd.concat([df_forecast,get_forecast_UDF(df[df['gran_tempp']==pdt])])\n    \ndf_forecast.to_csv(algo_path+\"/Out_of_sample_results_window_level (\"+datetime.today().strftime('%Y-%m-%d-%H-%M-%S')+\").csv\", index = False)\nlogger.info(\"Completed Backtesting\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"af0d3cfe-71e6-4dbf-868f-ad26ecb3a865","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Epoch 1/50\n5/5 - 11s - loss: 8.5920 - val_loss: 11.4404 - 11s/epoch - 2s/step\nEpoch 2/50\n5/5 - 1s - loss: 8.4101 - val_loss: 11.2437 - 503ms/epoch - 101ms/step\nEpoch 3/50\n5/5 - 0s - loss: 8.2612 - val_loss: 11.0925 - 256ms/epoch - 51ms/step\nEpoch 4/50\n5/5 - 0s - loss: 8.1023 - val_loss: 10.9719 - 244ms/epoch - 49ms/step\nEpoch 5/50\n5/5 - 0s - loss: 7.8669 - val_loss: 10.7268 - 253ms/epoch - 51ms/step\nEpoch 6/50\n5/5 - 0s - loss: 7.5902 - val_loss: 10.5274 - 230ms/epoch - 46ms/step\nEpoch 7/50\n5/5 - 0s - loss: 7.1800 - val_loss: 9.9962 - 239ms/epoch - 48ms/step\nEpoch 8/50\n5/5 - 0s - loss: 6.5588 - val_loss: 9.1540 - 225ms/epoch - 45ms/step\nEpoch 9/50\n5/5 - 0s - loss: 6.3789 - val_loss: 8.2168 - 248ms/epoch - 50ms/step\nEpoch 10/50\n5/5 - 0s - loss: 6.0736 - val_loss: 7.6581 - 216ms/epoch - 43ms/step\nEpoch 11/50\n5/5 - 0s - loss: 5.7955 - val_loss: 7.0617 - 221ms/epoch - 44ms/step\nEpoch 12/50\n5/5 - 2s - loss: 5.6518 - val_loss: 6.7362 - 2s/epoch - 491ms/step\nEpoch 13/50\n5/5 - 1s - loss: 5.5936 - val_loss: 6.5068 - 1s/epoch - 211ms/step\nEpoch 14/50\n5/5 - 1s - loss: 5.5360 - val_loss: 6.3726 - 603ms/epoch - 121ms/step\nEpoch 15/50\n5/5 - 0s - loss: 5.4548 - val_loss: 6.3068 - 290ms/epoch - 58ms/step\nEpoch 16/50\n5/5 - 0s - loss: 5.3709 - val_loss: 6.2534 - 241ms/epoch - 48ms/step\nEpoch 17/50\n5/5 - 0s - loss: 5.2931 - val_loss: 6.2060 - 214ms/epoch - 43ms/step\nEpoch 18/50\n5/5 - 0s - loss: 5.2126 - val_loss: 6.0765 - 233ms/epoch - 47ms/step\nEpoch 19/50\n5/5 - 0s - loss: 5.1259 - val_loss: 5.9220 - 229ms/epoch - 46ms/step\nEpoch 20/50\n5/5 - 0s - loss: 5.0390 - val_loss: 5.5998 - 219ms/epoch - 44ms/step\nEpoch 21/50\n5/5 - 0s - loss: 4.9252 - val_loss: 5.3745 - 223ms/epoch - 45ms/step\nEpoch 22/50\n5/5 - 0s - loss: 4.8199 - val_loss: 4.9944 - 219ms/epoch - 44ms/step\nEpoch 23/50\n5/5 - 0s - loss: 4.7118 - val_loss: 4.8050 - 226ms/epoch - 45ms/step\nEpoch 24/50\n5/5 - 0s - loss: 4.6465 - val_loss: 4.5727 - 176ms/epoch - 35ms/step\nEpoch 25/50\n5/5 - 0s - loss: 4.6112 - val_loss: 4.4296 - 168ms/epoch - 34ms/step\nEpoch 26/50\n5/5 - 0s - loss: 4.5594 - val_loss: 4.2270 - 70ms/epoch - 14ms/step\nEpoch 27/50\n5/5 - 0s - loss: 4.5224 - val_loss: 4.1161 - 54ms/epoch - 11ms/step\nEpoch 28/50\n5/5 - 0s - loss: 4.4859 - val_loss: 3.9966 - 57ms/epoch - 11ms/step\nEpoch 29/50\n5/5 - 0s - loss: 4.4288 - val_loss: 3.8536 - 52ms/epoch - 10ms/step\nEpoch 30/50\n5/5 - 0s - loss: 4.3973 - val_loss: 3.8252 - 51ms/epoch - 10ms/step\nEpoch 31/50\n5/5 - 0s - loss: 4.3715 - val_loss: 3.7573 - 54ms/epoch - 11ms/step\nEpoch 32/50\n5/5 - 0s - loss: 4.3327 - val_loss: 3.6446 - 53ms/epoch - 11ms/step\nEpoch 33/50\n5/5 - 0s - loss: 4.3227 - val_loss: 3.5147 - 56ms/epoch - 11ms/step\nEpoch 34/50\n5/5 - 0s - loss: 4.2906 - val_loss: 3.5508 - 84ms/epoch - 17ms/step\nEpoch 35/50\n5/5 - 0s - loss: 4.2762 - val_loss: 3.4462 - 56ms/epoch - 11ms/step\nEpoch 36/50\n5/5 - 0s - loss: 4.2652 - val_loss: 3.4249 - 60ms/epoch - 12ms/step\nEpoch 37/50\n5/5 - 0s - loss: 4.2754 - val_loss: 3.3786 - 58ms/epoch - 12ms/step\nEpoch 38/50\n5/5 - 0s - loss: 4.2553 - val_loss: 3.3611 - 60ms/epoch - 12ms/step\nEpoch 39/50\n5/5 - 0s - loss: 4.2422 - val_loss: 3.2705 - 74ms/epoch - 15ms/step\nEpoch 40/50\n5/5 - 0s - loss: 4.2406 - val_loss: 3.2470 - 56ms/epoch - 11ms/step\nEpoch 41/50\n5/5 - 0s - loss: 4.2262 - val_loss: 3.2862 - 75ms/epoch - 15ms/step\nEpoch 42/50\n5/5 - 0s - loss: 4.2235 - val_loss: 3.2179 - 120ms/epoch - 24ms/step\nEpoch 43/50\n5/5 - 0s - loss: 4.2181 - val_loss: 3.1768 - 80ms/epoch - 16ms/step\nEpoch 44/50\n5/5 - 0s - loss: 4.2062 - val_loss: 3.1642 - 56ms/epoch - 11ms/step\nEpoch 45/50\n5/5 - 0s - loss: 4.2009 - val_loss: 3.1061 - 59ms/epoch - 12ms/step\nEpoch 46/50\n5/5 - 1s - loss: 4.1911 - val_loss: 3.1191 - 504ms/epoch - 101ms/step\nEpoch 47/50\n5/5 - 8s - loss: 4.1907 - val_loss: 3.1076 - 8s/epoch - 2s/step\nEpoch 48/50\n5/5 - 0s - loss: 4.1975 - val_loss: 3.1026 - 396ms/epoch - 79ms/step\nEpoch 49/50\n5/5 - 0s - loss: 4.1817 - val_loss: 3.1106 - 244ms/epoch - 49ms/step\nEpoch 50/50\n5/5 - 0s - loss: 4.1791 - val_loss: 3.1037 - 217ms/epoch - 43ms/step\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Epoch 1/50\n5/5 - 11s - loss: 8.5920 - val_loss: 11.4404 - 11s/epoch - 2s/step\nEpoch 2/50\n5/5 - 1s - loss: 8.4101 - val_loss: 11.2437 - 503ms/epoch - 101ms/step\nEpoch 3/50\n5/5 - 0s - loss: 8.2612 - val_loss: 11.0925 - 256ms/epoch - 51ms/step\nEpoch 4/50\n5/5 - 0s - loss: 8.1023 - val_loss: 10.9719 - 244ms/epoch - 49ms/step\nEpoch 5/50\n5/5 - 0s - loss: 7.8669 - val_loss: 10.7268 - 253ms/epoch - 51ms/step\nEpoch 6/50\n5/5 - 0s - loss: 7.5902 - val_loss: 10.5274 - 230ms/epoch - 46ms/step\nEpoch 7/50\n5/5 - 0s - loss: 7.1800 - val_loss: 9.9962 - 239ms/epoch - 48ms/step\nEpoch 8/50\n5/5 - 0s - loss: 6.5588 - val_loss: 9.1540 - 225ms/epoch - 45ms/step\nEpoch 9/50\n5/5 - 0s - loss: 6.3789 - val_loss: 8.2168 - 248ms/epoch - 50ms/step\nEpoch 10/50\n5/5 - 0s - loss: 6.0736 - val_loss: 7.6581 - 216ms/epoch - 43ms/step\nEpoch 11/50\n5/5 - 0s - loss: 5.7955 - val_loss: 7.0617 - 221ms/epoch - 44ms/step\nEpoch 12/50\n5/5 - 2s - loss: 5.6518 - val_loss: 6.7362 - 2s/epoch - 491ms/step\nEpoch 13/50\n5/5 - 1s - loss: 5.5936 - val_loss: 6.5068 - 1s/epoch - 211ms/step\nEpoch 14/50\n5/5 - 1s - loss: 5.5360 - val_loss: 6.3726 - 603ms/epoch - 121ms/step\nEpoch 15/50\n5/5 - 0s - loss: 5.4548 - val_loss: 6.3068 - 290ms/epoch - 58ms/step\nEpoch 16/50\n5/5 - 0s - loss: 5.3709 - val_loss: 6.2534 - 241ms/epoch - 48ms/step\nEpoch 17/50\n5/5 - 0s - loss: 5.2931 - val_loss: 6.2060 - 214ms/epoch - 43ms/step\nEpoch 18/50\n5/5 - 0s - loss: 5.2126 - val_loss: 6.0765 - 233ms/epoch - 47ms/step\nEpoch 19/50\n5/5 - 0s - loss: 5.1259 - val_loss: 5.9220 - 229ms/epoch - 46ms/step\nEpoch 20/50\n5/5 - 0s - loss: 5.0390 - val_loss: 5.5998 - 219ms/epoch - 44ms/step\nEpoch 21/50\n5/5 - 0s - loss: 4.9252 - val_loss: 5.3745 - 223ms/epoch - 45ms/step\nEpoch 22/50\n5/5 - 0s - loss: 4.8199 - val_loss: 4.9944 - 219ms/epoch - 44ms/step\nEpoch 23/50\n5/5 - 0s - loss: 4.7118 - val_loss: 4.8050 - 226ms/epoch - 45ms/step\nEpoch 24/50\n5/5 - 0s - loss: 4.6465 - val_loss: 4.5727 - 176ms/epoch - 35ms/step\nEpoch 25/50\n5/5 - 0s - loss: 4.6112 - val_loss: 4.4296 - 168ms/epoch - 34ms/step\nEpoch 26/50\n5/5 - 0s - loss: 4.5594 - val_loss: 4.2270 - 70ms/epoch - 14ms/step\nEpoch 27/50\n5/5 - 0s - loss: 4.5224 - val_loss: 4.1161 - 54ms/epoch - 11ms/step\nEpoch 28/50\n5/5 - 0s - loss: 4.4859 - val_loss: 3.9966 - 57ms/epoch - 11ms/step\nEpoch 29/50\n5/5 - 0s - loss: 4.4288 - val_loss: 3.8536 - 52ms/epoch - 10ms/step\nEpoch 30/50\n5/5 - 0s - loss: 4.3973 - val_loss: 3.8252 - 51ms/epoch - 10ms/step\nEpoch 31/50\n5/5 - 0s - loss: 4.3715 - val_loss: 3.7573 - 54ms/epoch - 11ms/step\nEpoch 32/50\n5/5 - 0s - loss: 4.3327 - val_loss: 3.6446 - 53ms/epoch - 11ms/step\nEpoch 33/50\n5/5 - 0s - loss: 4.3227 - val_loss: 3.5147 - 56ms/epoch - 11ms/step\nEpoch 34/50\n5/5 - 0s - loss: 4.2906 - val_loss: 3.5508 - 84ms/epoch - 17ms/step\nEpoch 35/50\n5/5 - 0s - loss: 4.2762 - val_loss: 3.4462 - 56ms/epoch - 11ms/step\nEpoch 36/50\n5/5 - 0s - loss: 4.2652 - val_loss: 3.4249 - 60ms/epoch - 12ms/step\nEpoch 37/50\n5/5 - 0s - loss: 4.2754 - val_loss: 3.3786 - 58ms/epoch - 12ms/step\nEpoch 38/50\n5/5 - 0s - loss: 4.2553 - val_loss: 3.3611 - 60ms/epoch - 12ms/step\nEpoch 39/50\n5/5 - 0s - loss: 4.2422 - val_loss: 3.2705 - 74ms/epoch - 15ms/step\nEpoch 40/50\n5/5 - 0s - loss: 4.2406 - val_loss: 3.2470 - 56ms/epoch - 11ms/step\nEpoch 41/50\n5/5 - 0s - loss: 4.2262 - val_loss: 3.2862 - 75ms/epoch - 15ms/step\nEpoch 42/50\n5/5 - 0s - loss: 4.2235 - val_loss: 3.2179 - 120ms/epoch - 24ms/step\nEpoch 43/50\n5/5 - 0s - loss: 4.2181 - val_loss: 3.1768 - 80ms/epoch - 16ms/step\nEpoch 44/50\n5/5 - 0s - loss: 4.2062 - val_loss: 3.1642 - 56ms/epoch - 11ms/step\nEpoch 45/50\n5/5 - 0s - loss: 4.2009 - val_loss: 3.1061 - 59ms/epoch - 12ms/step\nEpoch 46/50\n5/5 - 1s - loss: 4.1911 - val_loss: 3.1191 - 504ms/epoch - 101ms/step\nEpoch 47/50\n5/5 - 8s - loss: 4.1907 - val_loss: 3.1076 - 8s/epoch - 2s/step\nEpoch 48/50\n5/5 - 0s - loss: 4.1975 - val_loss: 3.1026 - 396ms/epoch - 79ms/step\nEpoch 49/50\n5/5 - 0s - loss: 4.1817 - val_loss: 3.1106 - 244ms/epoch - 49ms/step\nEpoch 50/50\n5/5 - 0s - loss: 4.1791 - val_loss: 3.1037 - 217ms/epoch - 43ms/step\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Reading the latest Out_of_sample_results_window_level file based on timestamp\nall_files = [file for file in os.listdir(algo_path)]\nbacktesting_files = [file for file in all_files if \"Out_of_sample_results_window_level (\" in file]\nbacktesting_files = [file.replace(\".csv\",\"\") for file in backtesting_files]\nversion_dates = [datetime.strptime(x.split('(')[1].replace(')',''), '%Y-%m-%d-%H-%M-%S') for x in backtesting_files]\nmax_date = max(version_dates)\nmax_date = max_date.strftime('%Y-%m-%d-%H-%M-%S')\nreq_file_name = [x for x in backtesting_files if max_date in x]\nbacktesting_results_file_path = os.path.join(algo_path,req_file_name[0] + \".csv\")\nprint(backtesting_results_file_path)\n\n# Reading the results of backtesting\ndf = pd.read_csv(backtesting_results_file_path)\ndf = df[df[\"status\"] == \"success\"]\n\ndf[modeling_granularity_conf] = df[modeling_granularity_conf].astype(str)\ndf['ds'] = pd.to_datetime(df['ds'])\n    \n# Roll up the data at Modeling granularity window level\ndf_hyperparameters = best_hyperparam_results[gbcp + list(hyperparameters_conf)]\n\n# performance metrics\nper_met = ['status',\"test_flag\",\"window\",\"mape\",\"wmape\",\"bias\",\"tracking_signal\",\"mae\",\"rmse\"]\ndf_metrics = df[gbcp + per_met].drop_duplicates()\ndf_metrics1 = df_metrics.groupby(gbcp + ['test_flag','status'])[[\"mape\",\"wmape\",\"bias\",\"tracking_signal\",\"mae\",\"rmse\"]].mean().reset_index()\n\n# Remaining columns\nrem_cols = list(set(df.columns) - set(per_met+list(hyperparameters_conf))) + ['test_flag']\ndot_cols = [col for col in df.columns if \".\" in col] #to handle \".\"s\nfor col in dot_cols:\n    df.rename(columns = {col:col.replace(\".\",\"dot\")}, inplace = True)\n    rem_cols[rem_cols.index(col)] = col.replace(\".\",\"dot\")\nrem_df = df[rem_cols]\n\n\ngroup_cols = gbcp + ['ds','test_flag']\nagg_cols = list(set(rem_cols) - set(group_cols))\nexprs = {x: \"mean\" for x in agg_cols}\nrem_df1 = rem_df.groupby(group_cols).agg(exprs).reset_index()\ntemp_cols = [col[:-1] if 'avg(' in col else col for col in rem_df1.columns ]\ntemp_cols = [col.replace('avg(','') for col in temp_cols]\nrem_df1.columns = temp_cols\nfor col in dot_cols:\n    rem_df1.rename(columns = {col.replace(\".\",\"dot\"):col.replace(\"dot\",\".\")}, inplace = True)\n                            \n# combining all the data\ndf_forecast = rem_df1.merge(df_metrics1, on = gbcp + ['test_flag'], how='left')\ndf_forecast = df_forecast.merge(df_hyperparameters, on = gbcp , how='left')\ndf_forecast['algorithm'] = 'LSTM'\n\n# exporting the results\ndf_forecast.to_csv(algo_path+\"/Out_of_sample_evaluation_results (\"+datetime.today().strftime('%Y-%m-%d-%H-%M-%S')+\").csv\", index = False)\nlogger.info(\"Exported Out of sample evaluation results\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"04737f1d-ba1b-4543-83e5-f988342c39fd","inputWidgets":{},"title":"Rolling up the window level results to granularity x date"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/dbfs/mnt/solutionsadls_data/modeling_results_python/Modeling_Results/LSTM/Out_of_sample_results_window_level (2023-01-13-10-37-21).csv\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/dbfs/mnt/solutionsadls_data/modeling_results_python/Modeling_Results/LSTM/Out_of_sample_results_window_level (2023-01-13-10-37-21).csv\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Predicting future timeperiods\nThe following code assumes that the X-variables for the required future time periods are available for each modeling granularity\n\nUncomment the below cells if wants to predict the future, update the df respectively such that it contains entire historical data as well as idvs data for the required future forecast time periods"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fd0f712d-7d00-47aa-9b7e-9b5adc966877","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# broadcast_test_periods =  broadcast_variable_conf(4) # Provide the no. of timeperiods to forecast in the future"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bae009af-5114-4013-8a10-6ee3abaf6c11","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["## Reading the latest input file based on timestamp\n# all_files = [file for file in os.listdir(app_config['output_dir_path']+\"/Data_Processing/Missing_value_treatment\")]\n# missing_op_files = [file for file in all_files if \"Missing_value_treatment_results (\" in file]\n# missing_op_files = [file.replace(\".csv\",\"\") for file in missing_op_files]\n# version_dates = [datetime.strptime(x.split('(')[1].replace(')',''), '%Y-%m-%d-%H-%M-%S') for x in missing_op_files]\n# max_date = max(version_dates)\n# max_date = max_date.strftime('%Y-%m-%d-%H-%M-%S')\n# req_file_name = [x for x in missing_op_files if max_date in x]\n# missing_op_file_path = os.path.join(app_config['output_dir_path']+\"/Data_Processing/Missing_value_treatment\",req_file_name[0]+'.csv')\n## print(missing_op_file_path)\n\n## Reading the data\n# df = pd.read_csv(missing_op_file_path)\n## print(df.shape)\n\n# df.rename(columns = {ds_config:\"ds\", dv_config:\"y\"}, inplace = True)\n# df['ds'] = pd.to_datetime(df['ds'])\n# df[modeling_granularity_conf] = df[modeling_granularity_conf].astype(str)\n\n# # Broadcasting again with the \"Future forecast\" value since we won't be tracking the future forecast results\n# mlflow_tracking_check = broadcast_required_info(\"Future forecast\")\n# logger.info(\"Data which contains the future forecast periods is loaded\")\n\n# gbcp = list(modeling_granularity_conf)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7929ea30-31e4-4425-a9bd-c66066a4b863","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# df['gran_tempp'] = df[gbcp].astype(str).sum(axis=1)\n# unique_pdts = df['gran_tempp'].unique()\n# df_forecast = pd.DataFrame()\n# for pdt in unique_pdts:\n#     df_forecast = pd.concat([df_forecast,get_forecast_UDF(df[df['gran_tempp']==pdt])])\n            \n# del(df_forecast['test_flag_agg'])\n# df_forecast['algorithm'] = 'LSTM'"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"75c743bf-068c-476a-9040-f7055e43669e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# df_forecast.to_csv(algo_path + \"/Future_forecast_results (\"+datetime.today().strftime('%Y-%m-%d-%H-%M-%S')+\").csv\",index = False)\n# logger.info(\"Exported future forecast results\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"99ac5bcd-8a2a-4aaa-9145-0a87c506acf7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Exporting config file\nconfig_file_name = \"config_for_exp_id_\"+str(broadcast_tracking.value['mlflow_experiment_id']) + \" (\" +datetime.today().strftime('%Y-%m-%d-%H-%M-%S-%f')[:-3]+\").yml\"\nconfig_path1 = os.path.join(config_path,config_file_name)\nwith open(config_path1, 'w') as file:\n    yaml.dump(temp_config, file, default_flow_style=False,sort_keys=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f3f88afa-3bd3-4ba5-b628-a4f247fad3ae","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Move from tmp directory to req. location in datalake\nimport platform\nplat_sys = platform.system()\n\nif(plat_sys!='Windows'):\n    log_file = log_file.replace(' (', '\\ \\(').replace(')','\\)')\n    os.system('mv /tmp/{0} {1}'.format(log_file,logs_path))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b1f6b147-5c7b-48e5-9524-dc5218a52297","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"5.2 - Model evaluation, Retraining & Scoring","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":98246570296311}},"nbformat":4,"nbformat_minor":0}