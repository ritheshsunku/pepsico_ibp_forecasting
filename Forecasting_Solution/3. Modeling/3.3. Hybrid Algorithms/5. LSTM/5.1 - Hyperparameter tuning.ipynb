{"cells":[{"cell_type":"markdown","source":["## Timeseries Modeling - LSTM\n### Objective:\nThe objective of the notebook is to -\n* Backtest on all hyperparameters of LSTM provided in the config\n* Find the best set of hyperparameters using the metric provided in the config"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3114b9fc-7828-4869-a545-0cdbb50658a4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import yaml\nimport inspect\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom distutils.command.config import config\nfrom tqdm.auto import tqdm\nfrom datetime import timedelta\nfrom datetime import datetime\nimport mlflow\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nimport os\nimport logging\nimport dotsi\nimport tensorflow\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM,TimeDistributed,RepeatVector"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"db9e85f0-6d05-455f-9772-a4655fdc3f40","inputWidgets":{},"title":"Load relevant packages"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"One of the commands could not be parsed","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# logging part\np_dir = \"/tmp/\"\nlog_file = \"LSTM_hyperparameter_tuning\" + \" (\" +datetime.today().strftime('%Y-%m-%d-%H-%M-%S')+ \").log\"\n\n# LSTM - Hyperparameter tuning logs\nlogger = logging.getLogger('custom_log')\nlogger.setLevel(logging.DEBUG)\n\n# Applying necessary formatter\nfh = logging.FileHandler(p_dir+log_file)\nformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\nfh.setFormatter(formatter)\nlogger.addHandler(fh)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0c0b8522-90f9-48e0-87d3-17fab93579f1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"One of the commands could not be parsed","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Getting the default settings of hyperparameters. Used to check that user-provided hyperparameters must always be a subset of these.\ndef get_default_args(func) -> dict:\n    \"\"\"Function to get the default values of the hyperparameters for the given algorithm\n\n    Parameters\n    ----------\n    func : constructor of the respective algorithm\n        The name of the algorithm (Eg: Prophet,SARIMAX)\n\n    Returns\n    -------\n    dict\n        returns a dictionary of hyperparameters and the corresponding default values for the given algorithm\n    \"\"\"\n    signature = inspect.signature(func)\n    return {\n        k: v.default if v.default is not inspect.Parameter.empty else None\n        for k, v in signature.parameters.items()\n        if k != 'self'\n    }"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3ded48db-f67d-412a-ad56-d3904dfdb24b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"One of the commands could not be parsed","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Configurable Hyperparameters\nThe following are the possible hyperparameters that can be tuned for LSTM. The preferred hyperparameters, and their respective search spaces, over which tuning is to be done need to be mentioned in the config file."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1f7a8df8-c970-4e89-9529-ac9301c10bbf","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["default_hpps_compile = get_default_args(Sequential.compile)\ndefault_hpps_fit = get_default_args(Sequential.fit)\n    \nprint(\"Hyperparameters under compile\\n\",default_hpps_compile,\"\\n\\nHyperparameters under fit\\n\",default_hpps_fit)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d2101a02-80fb-461a-a858-4a146a61042c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"One of the commands could not be parsed","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Processing Config file\nDependent variable, date variable, modeling granularity & other related modeling details are provided in the form of a config file. Each TS Algorithm, and the related hyperparameter values to be tried, should be given in the config file."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2154c025-1057-46f2-a6d6-81e771ee9a6d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%run ../../../0_Config.ipynb"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"60af8778-946b-4a6b-a7d1-210886a146f3","inputWidgets":{},"title":"Reading the model configuration"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Notebook not found: solutions/02_Forecasting_Solution_Python/0_Config.ipynb. Notebooks can be specified via a relative path (./Notebook or ../folder/Notebook) or via an absolute path (/Abs/Path/to/Notebook). Make sure you are specifying the path correctly.\n\nStacktrace:\n  /solutions/02_Forecasting_Solution_Python/3. Modeling/3.3. Hybrid Algorithms/5. LSTM/5.1 - Hyperparameter tuning: python","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["logger.info(\"Config file read\")\nassert set(app_config[\"Algorithms\"][\"LSTM\"][\"Hyperparameters\"]['compile'].keys()).\\\n           issubset(set(default_hpps_compile.keys())),\\\n           'keys supplied by the user for the LSTM Algorithm under comiple method must be valid'\nassert set(app_config[\"Algorithms\"][\"LSTM\"][\"Hyperparameters\"]['fit'].keys()).\\\n           issubset(set(default_hpps_fit.keys())),\\\n           'keys supplied by the user for the LSTM Algorithm under fit method must be valid'\n\n# For exporting the config file\ntemp_config = app_config.copy()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ce120b5f-ae0b-4ce9-b395-71e08ea39f66","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"One of the commands could not be parsed","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["def frange(start,stop,step= 1):\n    l = []\n    i = start\n    while(i < stop):\n        l.append(round(i,len(str(step))))\n        i = i+step\n    return l\n\ndef drange(hyperparameters):\n    l=[]\n    for key in hyperparameters.keys():\n        val = hyperparameters[key]\n        if 'range' in val:\n            val = val.replace('range','frange')\n            new_str = 'total_list = '  + val\n            _locals = locals()\n            exec(new_str,globals(),_locals)\n            without_dup = list(set(_locals['total_list']))\n            hyperparameters[key] = without_dup\n    return hyperparameters"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"01660c3f-6c85-4990-8c3e-d9c7bdc667ae","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"One of the commands could not be parsed","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["fit_ = drange(app_config['Algorithms']['LSTM']['Hyperparameters']['fit'])\ncompile_ = drange(app_config['Algorithms']['LSTM']['Hyperparameters']['compile'])\nfor key in compile_.keys():\n    if(key in fit_.keys()):\n        fit_[key] = list(set(fit_[key]+compile_[key]))\n    else:\n        fit_[key] = list(compile_[key])\n    \nfit_new = {}\nfor key in fit_.keys():\n    temp = []\n    for val in fit_[key]:\n        if(type(val) == list):\n            val = str(val)\n        if((val!='None') and (val!='Null') and (val!=None)):\n            temp.append(val)\n    if(len(temp)>0):\n        fit_new[key] = temp\n        \nif('kwargs' in fit_new.keys()):\n    del fit_new['kwargs']\n    \nfor val in ['x','y','validation_data','kwargs']:\n    if(val in fit_new.keys()):\n        del fit_new[val]\n        \napp_config[\"Algorithms\"][\"LSTM\"][\"Hyperparameters\"] = fit_new"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fbd67444-386c-4791-b0e2-742caef0e829","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"One of the commands could not be parsed","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Create the algo directory for storing the results\noutput_directory = app_config['output_dir_path']\nroot_dir = \"Modeling_Results\"\nalgorithm = \"LSTM\"\nalgo_path = os.path.join(output_directory,root_dir,algorithm)\nif not os.path.exists(algo_path):\n    os.makedirs(algo_path)\nlogger.info(\"Created algorithm directory\")    \n\nlogs_path = os.path.join(output_directory,root_dir,'logs',algorithm)\nif not os.path.exists(logs_path):\n    os.makedirs(logs_path)\nlogger.info(\"Created logs directory\")\n\nconfig_path = os.path.join(app_config['output_dir_path'],\"Modeling_Results\",\"config\")\nif not os.path.exists(config_path):\n    os.makedirs(config_path)\nlogger.info(\"Created config directory\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6cac3d67-ae9b-4204-8b2c-6f2b737fe01f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"One of the commands could not be parsed","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Broadcasting the required variables\nVariables suffixed with \"_conf\" are taken from the config file"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4cffcf15-ceb5-4848-91a4-c484f8b66912","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["hyperparameters_conf = dict(app_config[\"Algorithms\"][\"LSTM\"][\"Hyperparameters\"])\n# print(hyperparameters_conf)\n\nmodeling_granularity_conf = app_config[\"modeling_granularity\"]\n# print(modeling_granularity_conf)\n\n# Rename Start date and DV config\ndv_config = app_config[\"dependent_variable\"]\nds_config = app_config[\"date_var\"]\n\n# pos and neg corr broadcast\ncorr_config = dict(app_config['Algorithms']['LSTM']['exogenous_variables'])\ncorr_config_broadcast = dotsi.Dict({\"value\":corr_config})\n\n# Eval metric broadcast\nbroadcast_metric = dotsi.Dict({\"value\":app_config[\"validation\"][\"metric\"]})\nbroadcast_tracking = dotsi.Dict({\"value\":app_config['tracking']})\nbroadcast_forecast_periods = dotsi.Dict({\"value\":app_config[\"Algorithms\"][\"LSTM\"][\"forecast_periods\"]})\nbroadcast_lookback_periods = dotsi.Dict({\"value\":app_config[\"Algorithms\"][\"LSTM\"][\"lookback_periods\"]})\nbroadcast_test_periods = dotsi.Dict({\"value\":app_config[\"validation\"][\"no_of_backtesting_test_periods\"]})\n\nbroadcast_granularity = dotsi.Dict({\"value\":modeling_granularity_conf})\nbroadcast_hyper_parameters = dotsi.Dict({\"value\":hyperparameters_conf})\n\nlogger.info(\"Broadcasted the required variables\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cefabbdb-dd5a-4cb9-bc6d-548ee044daf2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"One of the commands could not be parsed","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Reading feature selected output and using the significant variables as idvs in modeling\nfeature_selection_info = app_config['Algorithms']['LSTM']['feature_selection']\nbroadcast_use_features = dotsi.Dict({\"value\":feature_selection_info['use_feature_selected_idvs']})\nif(feature_selection_info['use_feature_selected_idvs']):\n    if(feature_selection_info['approach']=='lasso_cvglmnet'):\n        output_folder = app_config['output_dir_path']+\"/Feature_Selection/Lasso/\"\n    # Reading the latest input file based on timestamp\n    coeff_op_files = [file for file in os.listdir(output_folder)]\n    coeff_op_files = [file.replace(\".csv\",\"\") for file in coeff_op_files]\n    version_dates = [datetime.strptime(x.split('(')[1].replace(')',''), '%Y-%m-%d-%H-%M-%S') for x in coeff_op_files]\n    max_date = max(version_dates)\n    max_date = max_date.strftime('%Y-%m-%d-%H-%M-%S')\n    req_file_name = [x for x in coeff_op_files if max_date in x]\n    coeff_op_file_path = os.path.join(output_folder,req_file_name[0]+\".csv\")\n    print(coeff_op_file_path)\n\n    # Reading the data\n    coeff_df = pd.read_csv(coeff_op_file_path)\n    coeff_df = coeff_df[coeff_df['status']=='success']\n    # print(coeff_df.shape)\n    coeff_df[modeling_granularity_conf] = coeff_df[modeling_granularity_conf].astype(str)\n    idvs_len = len(feature_selection_info['must_have_idvs'])\n    if(idvs_len>0):\n        temp1 = coeff_df[modeling_granularity_conf].drop_duplicates()\n        temp1['temp'] = 1\n        temp2 = pd.DataFrame({'IDV':feature_selection_info['must_have_idvs']})\n        temp2['temp'] = 1\n        temp = temp1.join(temp2, on = 'temp', how ='left')\n        req_cols = modeling_granularity_conf + ['IDV']\n        coeff_df = coeff_df.drop_duplicates()\n    coeffs_broadcast = dotsi.Dict({\"value\":coeff_df})\n# display(coeff_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7f6e531d-e8c1-42c2-9fd4-76483ce42908","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"One of the commands could not be parsed","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Pandas UDF for Backtesting\nThe UDF gets executed in multiple worker nodes to parallelize the backtesting process. All the broadcasted variables are accessed within the UDF as and when required"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"53235143-a8d9-4772-8d04-36ae02c61986","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def get_prediction_UDF(df_data: pd.DataFrame)-> pd.DataFrame:\n    \"\"\"Function to perform backtesting for the given input data using the broadcasted information from the config file\n\n      Parameters\n      ----------\n      df_data : pd.DataFrame\n          The dataset containing values for all the required variables\n\n      Returns\n      -------\n      pd.DataFrame\n        Returns a dataframe with the granularity,date,hyperparameters,window and performance metrics\n    \"\"\"\n    try: \n        train_index_start = df_data[\"train_index_start\"].iloc[0]\n        train_index_end = df_data[\"train_index_end\"].iloc[0]\n        test_i = df_data[\"test_index_end\"].iloc[0]\n        window_no = df_data[\"window_no\"].iloc[0]\n        \n        df_data = df_data.sort_values(by=['ds'],ascending=True)\n        df_data = df_data.iloc[train_index_start:test_i].reset_index(drop=True)\n\n        # broadcast_granularity\n        broadcast_gran = broadcast_granularity.value\n        \n        # number of test periods to look\n        lookback_period = broadcast_lookback_periods.value\n        forecast_period = broadcast_forecast_periods.value\n        test_periods1 = int(broadcast_test_periods.value)\n        test_periods = test_periods1 - forecast_period + 1 \n\n        if(broadcast_use_features.value==True):\n            # Reading regressors from feature selection\n            coeffs_df = coeffs_broadcast.value\n            for x in broadcast_gran:\n                coeffs_df = coeffs_df[coeffs_df[x] == df_data[x].iloc[0]]\n            regressors = list(coeffs_df['IDV'].values)\n        else:\n            # Appending regressors based on the sign of correlation\n            corr_var = corr_config_broadcast.value\n            regressors = list(set(corr_var[\"positive_corr\"] + corr_var[\"negative_corr\"]+corr_var['uncertain_corr']))\n\n            temp_list1 = []\n            # Removing regressors based on the correlation\n            if(corr_var[\"consider_correlation\"]):   \n                for x in corr_var[\"positive_corr\"]:\n                    if(df_data[['y',x]].corr().iloc[0][1]<0):\n                        temp_list1.append(x)\n                for x in corr_var[\"negative_corr\"]:\n                    if (x not in temp_list1):\n                        if(df_data[['y',x]].corr().iloc[0][1]>0):\n                            temp_list1.append(x)   \n                regressors = list(set(regressors) - set(temp_list1))\n\n            # Checking for variance in the regressor\n            temp_list = []\n            if len(regressors)>0:\n                for ex_var in regressors:\n                    mean = df_data[ex_var].mean()\n                    std = df_data[ex_var].std()\n                    if mean == 0:\n                        if std <= 0.001:\n                            temp_list.append(ex_var)\n                    else:\n                        if abs(std/mean) <= 0.01:\n                            temp_list.append(ex_var)\n            regressors = list(set(regressors) - set(temp_list))\n                \n        # filtering for the required data\n        data = df_data[['y']+regressors].astype('float32')\n        values = data.values\n        n_vars = len(regressors) + 1\n        cols, names = list(), list()\n\n        # input sequence (t-n, ... t-1)\n        for i in range(lookback_period, 0, -1):\n            cols.append(data.shift(i))\n            names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n\n        # forecast sequence (t, t+1, ... t+n)\n        for i in range(0, forecast_period):\n            cols.append(data.shift(-i))\n            if i == 0:\n                names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n            else:\n                names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n\n        # put it all together\n        agg = pd.concat(cols, axis=1)\n        agg.columns = names\n\n        # drop rows with NaN values\n        if True:\n            agg.dropna(inplace=True)\n\n        # reframed = agg.copy()\n        cols_to_drop = [col for col in agg.columns if (\"var1\" not in col) & ((\"(t)\" in col) | (\"(t+\" in col))]\n        agg.drop(cols_to_drop, axis=1, inplace=True)\n\n        values = agg.values\n        train_val = values[:values.shape[0]-test_periods, :]\n        test_val = values[values.shape[0]-test_periods:, :]\n\n        # split into input and outputs\n        train_X, train_y = train_val[:, :-forecast_period], train_val[:, -forecast_period:]\n        test_X, test_y = test_val[:, :-forecast_period], test_val[:, -forecast_period:]\n\n        # reshape input to be 3D [samples, timesteps, features]\n        train_X = train_X.reshape((train_X.shape[0], lookback_period, n_vars))\n        test_X = test_X.reshape((test_X.shape[0], lookback_period, n_vars))\n\n        # Updating the default arguments with the parameters provided in the config\n        hp_config = broadcast_hyper_parameters.value\n        def_args = get_default_args(Sequential.compile)\n        for x in hp_config:\n            if(x in def_args.keys()):\n                temp_val = df_data[x].iloc[0]\n                if(type(temp_val)==str):\n                    if('[' in temp_val):\n                        temp_val = eval(temp_val)\n                def_args[x] = temp_val\n        if('kwargs' in def_args.keys()):\n            del def_args['kwargs']\n            \n        def_args_fit = get_default_args(Sequential.fit)\n        for x in hp_config:\n            if(x in def_args_fit.keys()):\n                temp_val = df_data[x].iloc[0]\n                if(type(temp_val)==str):\n                    if('[' in temp_val):\n                        temp_val = eval(temp_val)\n                def_args_fit[x] = temp_val\n        def_args_fit['x'] = train_X\n        def_args_fit['y'] = train_y\n        def_args_fit['validation_data'] = (test_X, test_y)\n        \n        # Calling the LSTM constructor with the hyperparameters of interest  \n        tensorflow.keras.utils.set_random_seed(1)\n        # design network\n        model = Sequential()\n        model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2]))) ## encoder \n        model.add(RepeatVector(forecast_period))\n        model.add(LSTM(50, activation='relu', return_sequences=True)) ## decoder\n        model.add(Dense(1))\n        model.compile(**def_args)\n        model.fit(**def_args_fit)\n        \n        ### predictions\n        # train_predict=model.predict(train_X)\n        test_predict=model.predict(test_X)\n        \n        # test period dates\n        test_dates = df_data.iloc[-test_periods1:]['ds'].values\n        test = pd.DataFrame()\n        for window in range(len(test_predict)):\n            test = pd.concat([test,pd.DataFrame({'ds':test_dates[window:forecast_period+window],\\\n                                                 'yhat':list(list(zip(*test_predict[window]))[0]),\\\n                                                 'window':np.repeat(window+1,forecast_period)})])\n        results_pd = pd.merge(test,df_data,how='left')\n        # results_pd = results_pd[broadcast_gran+['ds', 'y', 'yhat','window']].reset_index(drop = True)\n        # Sales or Quantity can't be negative hence\n        results_pd[\"yhat\"] = np.where(results_pd[\"yhat\"]<0,0,results_pd[\"yhat\"])\n        \n        # Eval. metrics calculation\n        # to handle erroneous results epsilon is set to 1.\n        epsilon = 1\n        temp_data1 = pd.DataFrame(index= range(1))\n        temp_data2 = pd.DataFrame()\n        for window in results_pd['window'].unique():\n            temp_data = results_pd[results_pd['window']==window]\n            y_pred = temp_data['yhat']\n            y_true = temp_data['y']\n\n            temp_data1['window'] = window\n            # Eval. metrics calculation\n            temp_data1['mape'] = np.mean(np.abs(y_true - y_pred) / np.maximum(np.abs(y_true), epsilon))*100  \n            temp_data1['wmape'] = np.sum(np.abs(y_true - y_pred)) / np.maximum(np.sum(np.abs(y_true)),epsilon)*100  \n            temp_data1['bias'] = np.mean((y_true - y_pred))  \n            temp_data1['tracking_signal'] = np.sum((y_true - y_pred)) / np.mean(np.abs(y_true - y_pred))\n            temp_data1['mae'] = mean_absolute_error(y_true, y_pred)\n            temp_data1['rmse']=np.sqrt(mean_squared_error(y_true, y_pred))\n            temp_data2 = pd.concat([temp_data2,temp_data1],ignore_index = True)\n        \n        # Grouping by granularity x date\n        results_pd = results_pd.groupby(broadcast_gran+['ds']).agg(y = ('y',np.mean), yhat = ('yhat',np.mean)).reset_index()\n        \n        # merging the performance metrics\n        results_pd['mape'] = temp_data2['mape'].mean()\n        results_pd['wmape'] = temp_data2['wmape'].mean()\n        results_pd['bias'] = temp_data2['bias'].mean()\n        results_pd['tracking_signal'] = temp_data2['tracking_signal'].mean()\n        results_pd['mae'] = temp_data2['mae'].mean()\n        results_pd['rmse'] = temp_data2['rmse'].mean()\n        \n        # To adhere to defined schema\n        for x in broadcast_gran:   \n            results_pd[x] = results_pd[x].astype(str)\n\n        # Append Hyperparameters used\n        for x in hp_config:\n            results_pd[x] = df_data[x].iloc[0]\n        \n        results_pd[\"window\"] = str(str(train_index_start)+\" \"+str(train_index_end)+\" \"+str(test_i)+\" \"+str(window_no))\n        results_pd['status'] = 'success'\n        return results_pd\n    \n    except Exception as e:\n        results_pd = pd.DataFrame(columns = [['ds', 'y', 'yhat','mape','wmape','bias',\\\n                                            'tracking_signal','mae','rmse','window']+\\\n                        list(broadcast_hyper_parameters.value.keys()) + ['status'] + broadcast_granularity.value],index = range(1))\n        results_pd[broadcast_granularity.value] = df_data[broadcast_granularity.value].head(1).reset_index(drop = True)\n        for x in broadcast_granularity.value:\n            results_pd[x] = results_pd[x].astype(str)\n        results_pd['status'] = str(e)\n        return results_pd"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c686a18c-ce00-428a-9dd8-a60ca75d23c4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"One of the commands could not be parsed","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Hyperparameter grid\nThe following function creates a cross product of all the hyperparameters provided in the config file and returns a hyperparamter grid"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"83f16e33-ba7b-4b59-aab5-af7502db75d0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Creating a hyperparameter grid using the set of provided hyperparameters in the config file\ndef create_hyperparam_space(hp_space: dict) -> pd.DataFrame:\n    \"\"\"Function to create a hyperparameter grid using the set of provided hyperparameters in the config file to be used for Hyperparameter tuning\n\n    Parameters\n    ----------\n    hp_space : dict\n        The set of provided hyperparameters in the config file\n\n    Returns\n    -------\n    pd.DataFrame\n        Returns a hyperparameter grid created from the set of provided hyperparameters\n    \"\"\"    \n    df_list = list()\n    for x in hp_space:\n      df_list.append(pd.DataFrame({x:hp_space[x]}))\n\n    space=df_list[0]\n\n    for x in df_list[1:]:\n        space = space.merge(x,how=\"cross\")\n\n    return space"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1e206e9b-e104-4063-bdc7-e552d42811d8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"One of the commands could not be parsed","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Backtesting parallelized using UDF\nBased on the Backtesting algorithm (sliding_window/expanding_window), training percentage & test periods specified in the config file, Hyperparameter tuning is performed"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9f764ca6-c6e6-4571-8432-abebb0d66dcd","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def hyperparameter_tuning(\n    algorithm: str,\n    train_data: pd.DataFrame,\n    hyperparam_space: dict,\n    modeling_granularity_conf: list,\n    train_percentage: float,\n    backtesting_test_periods: int,\n    test_periods: int,\n    stride: int,\n    modelling_func: pd.DataFrame = get_prediction_UDF,\n) -> pd.DataFrame:\n    \n    \"\"\"Function to perform the hyperparameter tuning\n\n    Parameters\n    ----------\n    algorithm : str\n        the algorithm with which the hyperparameter tuning is to be performed(sliding_window/expanding_window)\n    train_data : pd.DataFrame\n        the modelling dataset\n    hyperparam_space : dict\n        hyperparameter grid which was created using the set of hyperparameter from config file\n    modeling_granularity_conf : list\n        The list of the granularity variables at which the models will be built\n    train_percentage : float\n        The percentage of data points for the training\n    backtesting_test_periods : int\n        number of time periods to score in each iteration of backtesting\n    test_periods : int\n        number of time periods to score in final model building\n    stride : int\n        number of time periods to stride in each iteration of Backtesting\n    modelling_func : pd.DataFrame, optional\n        the function name to perform backtesting for the given dataset, by default get_prediction_UDF\n\n    Returns\n    -------\n    pd.DataFrame\n        Returns the dataframe containing the hyperparameter set of modelling granularity x hyperparameter set x window level\n    \"\"\"\n    if algorithm == \"expanding_window\":\n        gbcp = list(modeling_granularity_conf) + list(hyperparam_space.columns)\n        unique_skuXds = train_data[modeling_granularity_conf+[\"#total_weeks\"]].drop_duplicates().reset_index(drop = True)\n        \n        final_list = []\n        gran_len = len(modeling_granularity_conf)\n        for row1 in range(0,len(unique_skuXds)): \n            Total_weeks = unique_skuXds.loc[row1,'#total_weeks']\n            train_interval = int((Total_weeks-test_periods) * train_percentage)\n            j = 0\n            train_period_ends = Total_weeks-test_periods\n            for train_i in range(train_interval,train_period_ends,stride):\n                if(train_i+backtesting_test_periods <=train_period_ends):\n                    test_i = train_i+backtesting_test_periods\n                    final_list.append([unique_skuXds.iloc[row1,index] for index in range(gran_len)] + [0,train_i,train_i+backtesting_test_periods,j+1])\n                    j += 1\n                    \n        # create all windows combination.\n        df_windows = pd.DataFrame([tuple(x) for x in final_list],columns =modeling_granularity_conf+\\\n                                  ['train_index_start','train_index_end','test_index_end','window_no'])\n        f_df = train_data.merge(df_windows,on=modeling_granularity_conf,how=\"left\")\n        f_df['temppp'] = 1\n        hyperparam_space['temppp'] = 1\n        f_df = f_df.merge(hyperparam_space,on='temppp',how=\"left\")\n        f_df['gran_tempp'] = f_df[gbcp+[\"train_index_start\",\"train_index_end\",\"test_index_end\",\"window_no\"]].astype(str).sum(axis=1)\n        unique_pdts = f_df['gran_tempp'].unique()\n        new_results = pd.DataFrame()\n        for pdt in unique_pdts:\n            new_results = pd.concat([new_results,modelling_func(f_df[f_df['gran_tempp']==pdt])])\n        return new_results\n    \n    elif algorithm == \"sliding_window\":\n        gbcp = list(modeling_granularity_conf) + list(hyperparam_space.columns)\n        unique_skuXds = train_data[modeling_granularity_conf+[\"#total_weeks\"]].drop_duplicates().reset_index(drop = True)\n        \n        final_list = []\n        gran_len = len(modeling_granularity_conf)\n        for row1 in range(0,len(unique_skuXds)): \n            Total_weeks = unique_skuXds.loc[row1,'#total_weeks']\n            train_interval = int((Total_weeks-test_periods) * train_percentage)\n            j = 0\n            train_period_ends = Total_weeks-test_periods\n            train_index_start = 0\n            for train_i in range(train_interval,train_period_ends,stride):\n                if(train_i+backtesting_test_periods <=train_period_ends):\n                    test_i = train_i+backtesting_test_periods\n                    final_list.append([unique_skuXds.iloc[row1,index] for index in range(gran_len)] + \\\n                                      [train_index_start,train_i,train_i+backtesting_test_periods,j+1])\n                    j += 1\n                    train_index_start = train_index_start+stride\n                    \n        # create all windows combination.\n        df_windows = pd.DataFrame([tuple(x) for x in final_list],columns =modeling_granularity_conf+\\\n                                  ['train_index_start','train_index_end','test_index_end','window_no'])\n        f_df = train_data.merge(df_windows,on=modeling_granularity_conf,how=\"left\")\n        f_df['temppp'] = 1\n        hyperparam_space['temppp'] = 1\n        f_df = f_df.merge(hyperparam_space,on='temppp',how=\"left\")\n        f_df['gran_tempp'] = f_df[gbcp+[\"train_index_start\",\"train_index_end\",\"test_index_end\",\"window_no\"]].astype(str).sum(axis=1)\n        unique_pdts = f_df['gran_tempp'].unique()\n        new_results = pd.DataFrame()\n        for pdt in unique_pdts:\n            new_results = pd.concat([new_results,modelling_func(f_df[f_df['gran_tempp']==pdt])])\n        return new_results"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a0f2e132-1d1e-450c-bbd4-919c38f0e9ad","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"One of the commands could not be parsed","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Loading the latest Missing_value_treatment file\n##### Please update the reading path with the required data path if \"Missing value treatment\" was not run"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"98bf141c-0cee-4e7a-821b-393a3b5a2a64","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Reading the latest input file based on timestamp\nall_files = [file for file in os.listdir(app_config['output_dir_path']+\"/Data_Processing/Missing_value_treatment\")]\nmissing_op_files = [file for file in all_files if \"Missing_value_treatment_results (\" in file]\nmissing_op_files = [file.replace(\".csv\",\"\") for file in missing_op_files]\nversion_dates = [datetime.strptime(x.split('(')[1].replace(')',''), '%Y-%m-%d-%H-%M-%S') for x in missing_op_files]\nmax_date = max(version_dates)\nmax_date = max_date.strftime('%Y-%m-%d-%H-%M-%S')\nreq_file_name = [x for x in missing_op_files if max_date in x]\nmissing_op_file_path = os.path.join(app_config['output_dir_path']+\"/Data_Processing/Missing_value_treatment\",req_file_name[0] + \".csv\")\n# print(missing_op_file_path)\n\n# Reading the data\ndf = pd.read_csv(missing_op_file_path)\n# print(df.shape)\n\ndf.rename(columns = {ds_config:\"ds\", dv_config:\"y\"}, inplace = True)\nlogger.info(\"Data loaded\")\n\ndf['ds'] = pd.to_datetime(df['ds'])\ndf[modeling_granularity_conf] = df[modeling_granularity_conf].astype(str)\n\n# Getting the total number of weeks for each time series\ntemp_df = df.groupby(modeling_granularity_conf).agg({'ds':'count'}).rename(columns={'ds': '#total_weeks'}).reset_index()\ndf = df.merge(temp_df, on = modeling_granularity_conf ,how = \"left\")\n\n# 2. Create the hyperparameter space\nhpspace = create_hyperparam_space(hyperparameters_conf)\nlogger.info(\"Created hyperparameter grid\")\n\n# 4.  HP tuning based on algorithm of choice\ndf_f = hyperparameter_tuning(app_config['validation']['backtesting']['algorithm'],\\\n                             df,hpspace,modeling_granularity_conf,\\\n                             app_config['validation']['train_percentage'],\\\n                             app_config['validation']['no_of_backtesting_test_periods'],\\\n                             app_config['validation']['no_of_test_periods'],\\\n                             app_config['validation']['backtesting']['stride'],modelling_func = get_prediction_UDF)\n\ndf_f.to_csv(algo_path + \"/Backtesting_results_window_level (\" + datetime.today().strftime('%Y-%m-%d-%H-%M-%S')+\").csv\", index = False)\nlogger.info(\"Completed Backtesting\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"477089d4-3ffc-490d-ab8d-d3d730244b5a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"One of the commands could not be parsed","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Choosing the best hyperparameters"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5d3920a5-24a2-4c93-85a5-cd50bcbc184d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Reading the latest file based on timestamp\nall_files = [file for file in os.listdir(algo_path)]\nbacktesting_files = [file for file in all_files if \"Backtesting_results_window_level (\" in file]\nbacktesting_files = [file.replace(\".csv\",\"\") for file in backtesting_files]\nversion_dates = [datetime.strptime(x.split('(')[1].replace(')',''), '%Y-%m-%d-%H-%M-%S') for x in backtesting_files]\nmax_date = max(version_dates)\nmax_date = max_date.strftime('%Y-%m-%d-%H-%M-%S')\nreq_file_name = [x for x in backtesting_files if max_date in x]\nbacktesting_results_file_path = os.path.join(algo_path,req_file_name[0] + \".csv\")\n# print(backtesting_results_file_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d0fe8642-6703-409f-aab8-76afb0a446ad","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"One of the commands could not be parsed","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Reading the results of backtesting\ndf = pd.read_csv(backtesting_results_file_path)\ndf = df[df['status'] == 'success']\n# display(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"24056638-6110-4b31-bc67-1cec1967ee1a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"One of the commands could not be parsed","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Roll up the metrics at Modeling granularity x Hyperparameter space x window level\nwindow_level = modeling_granularity_conf + list(hyperparameters_conf.keys()) + [\"window\"]\n# print(window_level)\nwindow_level_results = df.groupby(window_level)[[\"mape\",\"wmape\",\"bias\",\"tracking_signal\",\"mae\",\"rmse\"]].min().reset_index()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a6350ea2-ffcb-4211-a6a7-efa9f6dd4c4a","inputWidgets":{},"title":"Choosing the best hyperparameters"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"One of the commands could not be parsed","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Roll up the metrics at Modeling granularity x Hyperparameter space level\nhyperparam_level = modeling_granularity_conf + list(hyperparameters_conf.keys())\nhyperparam_level_results = window_level_results.groupby(hyperparam_level)[[\"mape\",\"wmape\",\"bias\",\"tracking_signal\",\"mae\",\"rmse\"]].mean().reset_index()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5932262a-cfe8-4ea6-a95a-520803c376d2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"One of the commands could not be parsed","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["hyperparam_level_results.to_csv(algo_path + \"/Backtesting_results_hyperparameter_level (\" + datetime.today().strftime('%Y-%m-%d-%H-%M-%S')+\").csv\", index = False)\nlogger.info(\"Exported backtesting results\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f9c26b88-0942-48fa-86c9-0a43cfabcbd2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"One of the commands could not be parsed","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Pandas UDF for getting best hyperparameters & MLFlow tracking"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"38b81392-8332-4222-9b6e-54e4d8c76eda","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["hyperparam_level_results['algorithm'] = 'LSTM'\nhyperparam_level_results['result_type'] = 'backtesting'"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"402ba08c-9d27-4481-baa7-b4f57926b25b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"One of the commands could not be parsed","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_best_hyperparameters(final_hyperparam_df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Function for getting the best hyperparameters results for each modeling granularity along with MLFlow tracking using the broadcasted information from the config file\n\n    Parameters\n    ----------\n    final_hyperparam_df : pd.DataFrame\n        Dataframe containing the results of Backtesting \n\n    Returns\n    -------\n    pd.DataFrame\n        Dataset with best hyperparameter set for each modeling granularity\n    \"\"\"\n    try:\n        results_pd = {}\n\n        # Get the modeling granularity\n        broadcast_gran = broadcast_granularity.value\n        # Get the experiment id\n        tracking_value = broadcast_tracking.value.copy()\n\n        # To adhere to defined schema\n        for x in broadcast_gran:\n            results_pd[x] = final_hyperparam_df[x].astype(str).iloc[0]\n\n        hp_config = broadcast_hyper_parameters.value\n        granularity = broadcast_granularity.value\n        \n        if tracking_value[\"tracking_needed\"] == True:\n            if tracking_value[\"type\"] != \"Managed\":\n                if tracking_value[\"tracking_uri\"] is not None:\n                    mlflow.set_tracking_uri(\"file:\" + tracking_value[\"tracking_uri\"])\n                    experiment_id = mlflow.set_experiment(tracking_value[\"mlflow_experiment_id\"])\n                    tracking_value['mlflow_experiment_id'] = experiment_id.experiment_id\n\n            # Add MLFlow code here\n            with mlflow.start_run(experiment_id=tracking_value[\"mlflow_experiment_id\"]):\n                for x in [\"algorithm\",'result_type'] + broadcast_gran:\n                    mlflow.log_param(x, final_hyperparam_df[x].iloc[0])\n                for row in range(0, len(final_hyperparam_df)):\n                    with mlflow.start_run(experiment_id=tracking_value[\"mlflow_experiment_id\"], nested=True):\n                        for x in hp_config:\n                            mlflow.log_param(x, final_hyperparam_df[x].iloc[row])\n                        for x in [\"mape\",\"wmape\",\"bias\",\"tracking_signal\",\"mae\",\"rmse\"]:\n                            mlflow.log_metric(x, final_hyperparam_df[x].iloc[row])\n\n        metric = broadcast_metric.value\n\n        # Sort using the metric of interest\n        if metric in [\"wmape\", \"mape\", \"mad\", \"mae\", \"rmse\"]:\n            final_hyperparam_df = final_hyperparam_df.sort_values(\n                metric, ascending=True\n            )\n        elif metric in [\"tracking_signal\",\"bias\"]:\n            final_hyperparam_df.sort_values(metric, ascending=True,key=abs, inplace=True)\n        else:\n            final_hyperparam_df.sort_values(metric, ascending=True, inplace=True)\n\n        # Adding the best hyperparameter and related metrics\n        for x in hp_config:\n            results_pd[x] = final_hyperparam_df[x].iloc[0]\n\n        for x in [\"mape\", \"wmape\", \"bias\", \"tracking_signal\", \"mae\", \"rmse\"]:\n            results_pd[x] = final_hyperparam_df[x].iloc[0]\n\n        results_pd[\"status\"] = \"success\"\n        return pd.DataFrame.from_dict([results_pd])\n\n    except Exception as e:\n        results_pd = pd.DataFrame(\n            columns=[[\"mape\", \"wmape\", \"bias\", \"tracking_signal\", \"mae\", \"rmse\"]+ list(broadcast_hyper_parameters.value.keys())+ [\"status\"]+ broadcast_granularity.value],\\\n            index=range(1))\n        results_pd[broadcast_granularity.value] = final_hyperparam_df[broadcast_granularity.value].head(1).reset_index(drop=True)\n        for x in broadcast_granularity.value:\n            results_pd[x] = results_pd[x].astype(str)\n        results_pd[\"status\"] = str(e)\n        return results_pd"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d076cc54-931c-4977-801e-ee2eb97f8489","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"One of the commands could not be parsed","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["hyperparam_level_results['gran_tempp'] = hyperparam_level_results[modeling_granularity_conf].astype(str).sum(axis=1)\nunique_pdts = hyperparam_level_results['gran_tempp'].unique()\nbest_hyperparam_results = pd.DataFrame()\nfor pdt in unique_pdts:\n    best_hyperparam_results = pd.concat([best_hyperparam_results,get_best_hyperparameters(hyperparam_level_results[hyperparam_level_results['gran_tempp']==pdt])])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"60421bb2-7f9e-4d67-9614-31d7aa2fa330","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"One of the commands could not be parsed","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Writing the best hyperparameter results"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2088e5a8-f151-4247-92c0-91affb414828","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["best_hyperparam_results.to_csv(algo_path + \"/Best_hyperparameters (\" + datetime.today().strftime('%Y-%m-%d-%H-%M-%S')+\").csv\", index = False)\nlogger.info(\"Exported best hyperparameter results\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3d9f3c4f-c922-499e-9aee-db30f6e49b33","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"One of the commands could not be parsed","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Exporting config file\nconfig_file_name = \"config_for_exp_id_\"+str(broadcast_tracking.value['mlflow_experiment_id']) + \" (\" +datetime.today().strftime('%Y-%m-%d-%H-%M-%S-%f')[:-3]+\").yml\"\nconfig_path1 = os.path.join(config_path,config_file_name)\nwith open(config_path1, 'w') as file:\n    yaml.dump(temp_config, file, default_flow_style=False,sort_keys=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"19333e1b-fac7-4c22-b51f-81363a06d911","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"One of the commands could not be parsed","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Move from tmp directory to req. location in datalake\nimport platform\nplat_sys = platform.system()\n\nif(plat_sys!='Windows'):\n    log_file = log_file.replace(' (', '\\ \\(').replace(')','\\)')\n    os.system('mv /tmp/{0} {1}'.format(log_file,logs_path))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e169c8ac-b9a9-4776-bc52-393f79a0dc62","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"One of the commands could not be parsed","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"5.1 - Hyperparameter tuning","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":98246570296343}},"nbformat":4,"nbformat_minor":0}