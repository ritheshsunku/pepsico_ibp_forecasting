{"cells":[{"cell_type":"markdown","source":["### Objective:\nThe objective of the notebook is to -\n* Build the final model of Prophet algorithm using the best hyperparameter set identified using Backtesting and score the test set to get a performance metric\n* For forecasting future periods, we will re-train the model with the same hyperparameter set on the train + validation + test set to capture the patterns in the test set and then forecast future N periods"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"994e9d2c-9841-4322-84e8-b75c87a32892","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import yaml\nimport inspect\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom prophet import Prophet\nfrom prophet.make_holidays import make_holidays_df\nfrom distutils.command.config import config\nfrom tqdm.auto import tqdm\nfrom datetime import timedelta\nfrom datetime import datetime\nimport mlflow\nimport dotsi\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nimport os\nimport logging\nfrom prophet.utilities import regressor_coefficients"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"8afbc8bf-179f-4ffb-893e-37b23843914e","inputWidgets":{},"title":"Load relevant packages"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# logging part\np_dir = \"/tmp/\"\nlog_file = \"Prophet_model_eval_retraining_scoring\" + \" (\" +datetime.today().strftime('%Y-%m-%d-%H-%M-%S')+ \").log\"\n\nlogger = logging.getLogger('custom_log')\nlogger.setLevel(logging.DEBUG)\n\n# Applying necessary formatter\nfh = logging.FileHandler(p_dir+log_file, mode = 'a')\nformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\nfh.setFormatter(formatter)\nlogger.addHandler(fh)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6bb7221f-3b3e-4b37-b3e3-0759c2ef0efc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Getting the default settings of hyperparameters. Used to check that user-provided hyperparameters must always be a subset of these.\ndef get_default_args(func) -> dict:\n    \"\"\"Function to get the default values of the hyperparameters for the given algorithm\n\n    Parameters\n    ----------\n    func : constructor of the respective algorithm\n        The name of the algorithm (Eg: Prophet,SARIMAX)\n\n    Returns\n    -------\n    dict\n        returns a dictionary of hyperparameters and the corresponding default values for the given algorithm\n    \"\"\"\n    \n    signature = inspect.signature(func)\n    return {\n        k: v.default if v.default is not inspect.Parameter.empty else None\n        for k, v in signature.parameters.items()\n        if k != 'self'\n    }\ndefault_hpps = get_default_args(Prophet)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"497f0fb9-2e94-40d4-9fea-50103670624e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def broadcast_holidays(\n    config_holidays: dict,\n    year_list: list =[2018, 2019, 2020, 2021, 2022],\n    country_name: str =\"US\",\n    holiday_lower_window: int =7,\n    holiday_upper_window: int =7,\n) -> pd.DataFrame:\n    \"\"\"Function to return the dataframe of holidays for the given time period using Prophet's make_holidays_df()\n\n    Parameters\n    ----------\n    config_holidays : dict\n        the additional list of holidays and its respective dates provided by the user in config file\n    year_list : list, optional\n        the list of years for which we need the holidays , by default [2018, 2019, 2020, 2021, 2022]\n    country_name : str, optional\n        Name of the country based on which holidays can be decided, by default \"US\"\n    holiday_lower_window : int, optional\n        lower limit of the window, by default 7\n    holiday_upper_window : int, optional\n        upper limit of the window, by default 7\n\n    Returns\n    -------\n    pd.DataFrame\n        Returns a dataframe of holidays for the given time period\n    \"\"\"\n    holidays = make_holidays_df(year_list, country_name)\n    # Add window\n    holidays['lower_window'] = -holiday_lower_window\n    holidays['upper_window'] = holiday_upper_window\n    \n    # Adding additional holidays\n    if config_holidays is not None:\n        for ad_hol in config_holidays.keys():\n            temp_df = pd.DataFrame({'holiday':ad_hol,\n                                    'ds': pd.to_datetime(config_holidays[ad_hol]['ds']),\n                                    'lower_window': -holiday_lower_window,\n                                    'upper_window': holiday_upper_window})\n            holidays = pd.concat([holidays,temp_df])\n    \n    # Dropping duplicates if exists any\n    holidays = holidays.drop_duplicates().reset_index(drop = True)\n    return holidays"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"db9d9d5a-e471-43eb-a09d-33b10a2f3449","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%run ../../../0_Config.ipynb"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"46ba30a3-e79b-4225-b6ff-ec6906191e69","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["logger.info(\"Config file read\")\nassert set(app_config[\"Algorithms\"][\"Prophet\"][\"Hyperparameters\"].keys()).issubset(set(default_hpps.keys())),\\\n           'keys supplied by the user for the Prophet Algorithm under Hyperparameters must be valid'\n\n# For exporting the config file\ntemp_config = app_config.copy()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ee51d934-92bd-4d7f-8d89-559a85da0a13","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def frange(start,stop,step= 1):\n    l = []\n    i = start\n    while(i < stop):\n        l.append(round(i,len(str(step))))\n        i = i+step\n    return l\n\ndef drange(hyperparameters):\n    l=[]\n    for key in hyperparameters.keys():\n        val = hyperparameters[key]\n        if 'range' in val:\n            val = val.replace('range','frange')\n            new_str = 'total_list = '  + val\n            _locals = locals()\n            exec(new_str,globals(),_locals)\n            without_dup = list(set(_locals['total_list']))\n            hyperparameters[key] = without_dup\n    return hyperparameters"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4770594f-ed58-4566-ac60-37ecf326555b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["fit_ = app_config[\"Algorithms\"][\"Prophet\"][\"Hyperparameters\"]\nfit_new = {}\nfor key in fit_.keys():\n    temp = []\n    for val in fit_[key]:\n        if(type(val) == list):\n            val = str(val)\n        if((val!='None') and (val!='Null') and (val!=None)):\n            temp.append(val)\n    if(len(temp)>0):\n        fit_new[key] = temp\n        \napp_config[\"Algorithms\"][\"Prophet\"][\"Hyperparameters\"] = fit_new"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"364e5d2f-bb62-4714-a9fc-917b4c53d69c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Create the algo and logs directory for storing the results\noutput_directory = app_config['output_dir_path']\nroot_dir = \"Modeling_Results\"\nalgorithm = \"Prophet\"\nalgo_path = os.path.join(output_directory,root_dir,algorithm)\nif not os.path.exists(algo_path):\n    os.makedirs(algo_path)\nlogger.info(\"Created algorithm directory\")    \n\nlogs_path = os.path.join(output_directory,root_dir,'logs',algorithm)\nif not os.path.exists(logs_path):\n    os.makedirs(logs_path)\n    \nlogger.info(\"Created logs directory\")\n\nconfig_path = os.path.join(app_config['output_dir_path'],\"Modeling_Results\",\"config\")\nif not os.path.exists(config_path):\n    os.makedirs(config_path)\n    \nlogger.info(\"Created config directory\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8d22e37a-9abb-40be-9eab-b2e6557e8097","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["hyperparameters_conf = dict(app_config[\"Algorithms\"][\"Prophet\"][\"Hyperparameters\"])\n# print(hyperparameters_conf)\n\nmodeling_granularity_conf = app_config[\"modeling_granularity\"]\n# print(modeling_granularity_conf)\n\n# Rename Start date and DV config\ndv_config = app_config[\"dependent_variable\"]\nds_config = app_config[\"date_var\"]\n\n# pos and neg corr broadcast\ncorr_config = dict(app_config['Algorithms']['Prophet']['exogenous_variables'])\ncorr_config_broadcast = dotsi.Dict({\"value\":corr_config})\n\n# Eval metric broadcast\nbroadcast_metric = dotsi.Dict({\"value\":app_config['validation']['metric']})\nbroadcast_test_periods = dotsi.Dict({\"value\":app_config[\"validation\"][\"no_of_test_periods\"]})\nbroadcast_regressor_mode = dotsi.Dict({\"value\":app_config[\"Algorithms\"][\"Prophet\"][\"regressor_mode\"]})\nbroadcast_tracking = dotsi.Dict({\"value\":app_config['tracking']})\nmlflow_tracking_check = dotsi.Dict({\"value\":\"Out of Sample\"})\n# ===================================================================================\n\n# Broadcasting\nif app_config[\"Algorithms\"][\"Prophet\"][\"Holidays\"][\"include_holidays\"] == True:\n    aa = app_config[\"Algorithms\"][\"Prophet\"][\"Holidays\"]\n    holidays_broadcast = broadcast_holidays(aa['additional_holidays'],aa['years'],aa['country'],aa['holiday_lower_window'],aa['holiday_upper_window'])\n    holidays_broadcast = dotsi.Dict({\"value\":holidays_broadcast})\nelse:\n    holidays_broadcast = dotsi.Dict({\"value\":None})\n    \nbroadcast_regressors = dotsi.Dict({\"value\":list(set(corr_config['positive_corr']+corr_config['negative_corr']+corr_config['uncertain_corr']))})\nbroadcast_granularity =dotsi.Dict({\"value\":modeling_granularity_conf})\nbroadcast_hyper_parameters = dotsi.Dict({\"value\":hyperparameters_conf})\nbroadcast_agg_metrics_req = dotsi.Dict({\"value\":app_config[\"validation\"][\"agg_metrics_req\"]})\n\nlogger.info(\"Broadcasted the required variables\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a06d2154-c076-49d8-bb35-d7a3809fd115","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Reading the latest file based on timestamp\nall_files = [file for file in os.listdir(algo_path)]\nbest_hyp_files = [file for file in all_files if \"Best_hyperparameters (\" in file]\nbest_hyp_files = [file.replace(\".csv\",\"\") for file in best_hyp_files]\nversion_dates = [datetime.strptime(x.split('(')[1].replace(')',''), '%Y-%m-%d-%H-%M-%S') for x in best_hyp_files]\nmax_date = max(version_dates)\nmax_date = max_date.strftime('%Y-%m-%d-%H-%M-%S')\nreq_file_name = [x for x in best_hyp_files if max_date in x]\nbest_hyp_param_results_file_path = os.path.join(algo_path,req_file_name[0]+\".csv\")\nprint(best_hyp_param_results_file_path)\n\nbest_hyperparam_results = pd.read_csv(best_hyp_param_results_file_path)\nbest_hyperparam_results = best_hyperparam_results[best_hyperparam_results['status']=='success'].reset_index(drop = True)\nbest_hyperparam_results[modeling_granularity_conf] = best_hyperparam_results[modeling_granularity_conf].astype(str)\nbest_hyperparam_results.replace(['true'],True, inplace = True)\nbest_hyperparam_results.replace(['false'],False, inplace = True)\nbest_hyperparam_results_broadcast = dotsi.Dict({\"value\":best_hyperparam_results})\nlogger.info(\"Read the best hyperparamter results\")\nbest_hyperparam_results"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b0058222-f420-49c7-a6ed-6593783f8270","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/dbfs/mnt/solutionsadls_data/modeling_results_python/Modeling_Results/Prophet/Best_hyperparameters (2023-01-13-05-34-40).csv\nOut[18]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/dbfs/mnt/solutionsadls_data/modeling_results_python/Modeling_Results/Prophet/Best_hyperparameters (2023-01-13-05-34-40).csv\nOut[18]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Div_No</th>\n      <th>Store_No</th>\n      <th>Base_UPC</th>\n      <th>seasonality_prior_scale</th>\n      <th>changepoint_prior_scale</th>\n      <th>yearly_seasonality</th>\n      <th>mape</th>\n      <th>wmape</th>\n      <th>bias</th>\n      <th>tracking_signal</th>\n      <th>mae</th>\n      <th>rmse</th>\n      <th>status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24</td>\n      <td>8</td>\n      <td>2200015934</td>\n      <td>0.9</td>\n      <td>0.01</td>\n      <td>True</td>\n      <td>71.544410</td>\n      <td>42.661999</td>\n      <td>0.026536</td>\n      <td>-0.227906</td>\n      <td>2.490881</td>\n      <td>2.929188</td>\n      <td>success</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>24</td>\n      <td>14</td>\n      <td>4000042206</td>\n      <td>0.9</td>\n      <td>0.01</td>\n      <td>True</td>\n      <td>93.907670</td>\n      <td>73.514311</td>\n      <td>-2.813054</td>\n      <td>-0.025078</td>\n      <td>8.825539</td>\n      <td>9.877684</td>\n      <td>success</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>24</td>\n      <td>15</td>\n      <td>4000046410</td>\n      <td>0.9</td>\n      <td>0.01</td>\n      <td>True</td>\n      <td>147.603857</td>\n      <td>88.713018</td>\n      <td>-0.830174</td>\n      <td>-0.648747</td>\n      <td>6.231397</td>\n      <td>7.067459</td>\n      <td>success</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>24</td>\n      <td>17</td>\n      <td>4000000032</td>\n      <td>0.9</td>\n      <td>0.01</td>\n      <td>True</td>\n      <td>89.705656</td>\n      <td>63.424032</td>\n      <td>-1.080522</td>\n      <td>0.515436</td>\n      <td>5.768601</td>\n      <td>6.929459</td>\n      <td>success</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>24</td>\n      <td>53</td>\n      <td>4000000263</td>\n      <td>0.5</td>\n      <td>0.01</td>\n      <td>True</td>\n      <td>154.538461</td>\n      <td>98.742796</td>\n      <td>18.991775</td>\n      <td>3.870443</td>\n      <td>22.795517</td>\n      <td>24.429324</td>\n      <td>success</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>24</td>\n      <td>55</td>\n      <td>4000000051</td>\n      <td>0.9</td>\n      <td>0.01</td>\n      <td>True</td>\n      <td>193.146757</td>\n      <td>130.875273</td>\n      <td>0.309056</td>\n      <td>-0.552168</td>\n      <td>4.642972</td>\n      <td>6.145017</td>\n      <td>success</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>24</td>\n      <td>55</td>\n      <td>4000005851</td>\n      <td>0.5</td>\n      <td>0.01</td>\n      <td>True</td>\n      <td>371.128422</td>\n      <td>226.021793</td>\n      <td>-2.182556</td>\n      <td>1.244516</td>\n      <td>4.992277</td>\n      <td>5.922593</td>\n      <td>success</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Div_No</th>\n      <th>Store_No</th>\n      <th>Base_UPC</th>\n      <th>seasonality_prior_scale</th>\n      <th>changepoint_prior_scale</th>\n      <th>yearly_seasonality</th>\n      <th>mape</th>\n      <th>wmape</th>\n      <th>bias</th>\n      <th>tracking_signal</th>\n      <th>mae</th>\n      <th>rmse</th>\n      <th>status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24</td>\n      <td>8</td>\n      <td>2200015934</td>\n      <td>0.9</td>\n      <td>0.01</td>\n      <td>True</td>\n      <td>71.544410</td>\n      <td>42.661999</td>\n      <td>0.026536</td>\n      <td>-0.227906</td>\n      <td>2.490881</td>\n      <td>2.929188</td>\n      <td>success</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>24</td>\n      <td>14</td>\n      <td>4000042206</td>\n      <td>0.9</td>\n      <td>0.01</td>\n      <td>True</td>\n      <td>93.907670</td>\n      <td>73.514311</td>\n      <td>-2.813054</td>\n      <td>-0.025078</td>\n      <td>8.825539</td>\n      <td>9.877684</td>\n      <td>success</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>24</td>\n      <td>15</td>\n      <td>4000046410</td>\n      <td>0.9</td>\n      <td>0.01</td>\n      <td>True</td>\n      <td>147.603857</td>\n      <td>88.713018</td>\n      <td>-0.830174</td>\n      <td>-0.648747</td>\n      <td>6.231397</td>\n      <td>7.067459</td>\n      <td>success</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>24</td>\n      <td>17</td>\n      <td>4000000032</td>\n      <td>0.9</td>\n      <td>0.01</td>\n      <td>True</td>\n      <td>89.705656</td>\n      <td>63.424032</td>\n      <td>-1.080522</td>\n      <td>0.515436</td>\n      <td>5.768601</td>\n      <td>6.929459</td>\n      <td>success</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>24</td>\n      <td>53</td>\n      <td>4000000263</td>\n      <td>0.5</td>\n      <td>0.01</td>\n      <td>True</td>\n      <td>154.538461</td>\n      <td>98.742796</td>\n      <td>18.991775</td>\n      <td>3.870443</td>\n      <td>22.795517</td>\n      <td>24.429324</td>\n      <td>success</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>24</td>\n      <td>55</td>\n      <td>4000000051</td>\n      <td>0.9</td>\n      <td>0.01</td>\n      <td>True</td>\n      <td>193.146757</td>\n      <td>130.875273</td>\n      <td>0.309056</td>\n      <td>-0.552168</td>\n      <td>4.642972</td>\n      <td>6.145017</td>\n      <td>success</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>24</td>\n      <td>55</td>\n      <td>4000005851</td>\n      <td>0.5</td>\n      <td>0.01</td>\n      <td>True</td>\n      <td>371.128422</td>\n      <td>226.021793</td>\n      <td>-2.182556</td>\n      <td>1.244516</td>\n      <td>4.992277</td>\n      <td>5.922593</td>\n      <td>success</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Reading feature selected output and using the significant variables as idvs in modeling\nfeature_selection_info = app_config['Algorithms']['Prophet']['feature_selection']\nbroadcast_use_features = dotsi.Dict({\"value\":feature_selection_info['use_feature_selected_idvs']})\nif(feature_selection_info['use_feature_selected_idvs']):\n    if(feature_selection_info['approach']=='lasso_cvglmnet'):\n        output_folder = app_config['output_dir_path']+\"/Feature_Selection/Lasso/\"\n    # Reading the latest input file based on timestamp\n    coeff_op_files = [file for file in os.listdir(output_folder)]\n    coeff_op_files = [file.replace(\".csv\",\"\") for file in coeff_op_files]\n    version_dates = [datetime.strptime(x.split('(')[1].replace(')',''), '%Y-%m-%d-%H-%M-%S') for x in coeff_op_files]\n    max_date = max(version_dates)\n    max_date = max_date.strftime('%Y-%m-%d-%H-%M-%S')\n    req_file_name = [x for x in coeff_op_files if max_date in x]\n    coeff_op_file_path = os.path.join(output_folder,req_file_name[0] + \".csv\")\n    print(coeff_op_file_path)\n\n    # Reading the data\n    coeff_df = pd.read_csv(coeff_op_file_path)\n    coeff_df = coeff_df[coeff_df['status']=='success']\n    # print(coeff_df.shape)\n    coeff_df[modeling_granularity_conf] = coeff_df[modeling_granularity_conf].astype(str)\n    idvs_len = len(feature_selection_info['must_have_idvs'])\n    if(idvs_len>0):\n        temp1 = coeff_df[modeling_granularity_conf].drop_duplicates()\n        temp1['temp'] = 1\n        temp2 = pd.DataFrame({'IDV':feature_selection_info['must_have_idvs']})\n        temp2['temp'] = 1\n        temp = temp1.join(temp2, on = 'temp', how ='left')\n        req_cols = modeling_granularity_conf + ['IDV']\n        coeff_df = coeff_df.drop_duplicates()\n    coeffs_broadcast = dotsi.Dict({\"value\":coeff_df})\n    broadcast_regressors = dotsi.Dict({\"value\":list(coeff_df['IDV'].unique())})\n# display(coeff_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"89a85881-9d16-477c-b2c1-de8f54c81441","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/dbfs/mnt/solutionsadls_data/modeling_results_python/Feature_Selection/Lasso/lasso_feature_selection_results (2023-01-13-04-31-21).csv\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/dbfs/mnt/solutionsadls_data/modeling_results_python/Feature_Selection/Lasso/lasso_feature_selection_results (2023-01-13-04-31-21).csv\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_forecast_UDF(df_data: pd.DataFrame)-> pd.DataFrame:\n    \"\"\"Function to perform final model building using the train data and score on the test data utilizing the broadcasted details from the config file\n\n    Parameters\n    ----------\n    df_data : pd.DataFrame\n        The dataset containing values for all the required variables\n\n    Returns\n    -------\n    pd.DataFrame\n      Returns a dataframe with the granularity,date,independent variables contributions if any and performance metrics for the training and the testing set\n    \"\"\"\n    try:\n        test_periods = int(broadcast_test_periods.value)\n        if(broadcast_agg_metrics_req.value == True):\n            train_index_start = df_data[\"train_index_start\"].iloc[0]\n            train_index_end = df_data[\"train_index_end\"].iloc[0]\n            test_i = df_data[\"test_index_end\"].iloc[0]\n            window_no = str(str(train_index_start)+\" \"+str(train_index_end)+\" \"+str(test_i)+\" \"+str(df_data[\"window_no\"].iloc[0]))\n        else:\n            train_index_end = len(df_data) - test_periods\n            test_i = len(df_data)\n            window_no = str(1)\n            \n        df_data = df_data.sort_values(by=['ds'],ascending=True)\n        hpt = best_hyperparam_results_broadcast.value\n        regressor_mode = broadcast_regressor_mode.value\n\n        # broadcast_granularity\n        broadcast_gran = broadcast_granularity.value\n\n        # get best hyperparameters for the given modeling granularity\n        for x in list(broadcast_gran):\n            hpt = hpt[hpt[x] == df_data[x].iloc[0]]\n\n        # Train - test split\n        train = df_data.iloc[:train_index_end]\n        test = df_data.iloc[train_index_end:test_i]\n        \n        # Updating the default arguments with the parameters provided in the config\n        hp_config = list(broadcast_hyper_parameters.value)\n        def_args = get_default_args(Prophet)\n        for x in hp_config:\n            def_args[x] = hpt[x].iloc[0]\n        if holidays_broadcast.value is not None:\n            def_args[\"holidays\"] = holidays_broadcast.value\n\n        if(regressor_mode not in ['additive','multiplicative']):\n            regressor_mode = def_args['seasonality_mode'] \n        # Calling the Prophet constructor with the hyperparameters of interest  \n        m = Prophet(**def_args)\n        \n        if(broadcast_use_features.value==True):\n            # Reading regressors from feature selection\n            coeffs_df = coeffs_broadcast.value\n            for x in broadcast_gran:\n                coeffs_df = coeffs_df[coeffs_df[x] == df_data[x].iloc[0]]\n            regressors = list(coeffs_df['IDV'].values)\n            temp_list = list(set(broadcast_regressors.value) - set(regressors))\n        else:\n            # Appending regressors based on the sign of correlation\n            corr_var = corr_config_broadcast.value\n            regressors = list(set(corr_var[\"positive_corr\"] + corr_var[\"negative_corr\"]+corr_var['uncertain_corr']))\n\n            temp_list1 = []\n            # Removing regressors based on the correlation\n            if(corr_var[\"consider_correlation\"]):   \n                for x in corr_var[\"positive_corr\"]:\n                    if(train[['y',x]].corr().iloc[0][1]<0):\n                        temp_list1.append(x)\n                for x in corr_var[\"negative_corr\"]:\n                    if (x not in temp_list1):\n                        if(train[['y',x]].corr().iloc[0][1]>0):\n                            temp_list1.append(x)   \n                regressors = list(set(regressors) - set(temp_list1))\n                \n            # Checking for variance in the regressor\n            temp_list2 = []\n            if len(regressors)>0:\n                for ex_var in regressors:  \n                    mean = train[ex_var].mean()\n                    std = train[ex_var].std()\n                    if mean == 0:\n                        if std <= 0.001:\n                            temp_list2.append(ex_var)\n                    else:\n                        if abs(std/mean) <= 0.01:\n                            temp_list2.append(ex_var)\n                            \n            regressors = list(set(regressors) - set(temp_list2)) \n            temp_list = temp_list1+temp_list2\n            \n        for var in regressors:\n            m.add_regressor(var,mode = regressor_mode)\n        m.fit(train)\n\n        test_res = m.predict(test)\n        test_res[['test_flag','test_flag_agg']] = 1\n        train_res = m.predict(train)\n        train_res['test_flag'] = np.where(train_res['ds']<df_data.iloc[-test_periods:]['ds'].min(),0,1)\n        train_res['test_flag_agg'] = 0\n        forecast_pd = pd.concat([train_res,test_res],ignore_index = True)\n        \n        seasonal_cols = ['yearly','weekly','daily']\n        for var in seasonal_cols:\n            if(var not in forecast_pd.columns):\n                forecast_pd[var] = 0\n        if holidays_broadcast.value is not None:\n            holidays_list = list(m.train_holiday_names.values) + ['holidays']\n        else:\n            holidays_list = []\n            temp_list.append(\"holidays\")\n        results_pd = forecast_pd[['ds', 'yhat', 'yhat_upper','yhat_lower','trend','test_flag','test_flag_agg']\\\n                               +regressors+seasonal_cols+holidays_list]\n\n        # Contribution Calculation\n        if(m.seasonality_mode == 'multiplicative'):\n            for var in ['yearly','weekly','daily']:\n                if(var in results_pd.columns):\n                    results_pd[var] = results_pd[var]*results_pd['trend']\n            for var in holidays_list :\n                results_pd[var] = results_pd[var]*results_pd['trend']\n\n        if(len(regressors)>0):\n            reg_coeff = regressor_coefficients(m)\n            if(reg_coeff['regressor_mode'].unique()[0]=='multiplicative'):\n                for var in regressors:\n                    results_pd[var] = results_pd[var]*results_pd['trend']\n\n        results_pd = pd.merge(results_pd, df_data[['y','ds']+broadcast_gran], how = \"left\",on = \"ds\")\n        # Sales or Quantity can't be negative hence\n        results_pd[\"yhat\"] = np.where(results_pd[\"yhat\"]<0,0,results_pd[\"yhat\"])\n        results_pd[\"yhat_upper\"] = np.where(results_pd[\"yhat_upper\"]<0,0,results_pd[\"yhat_upper\"])\n        results_pd[\"yhat_lower\"] = np.where(results_pd[\"yhat_lower\"]<0,0,results_pd[\"yhat_lower\"])\n\n        # to handle erroneous results epsilon is set to 1.\n        epsilon = 1\n\n        temp_data1 = pd.DataFrame(index= range(1))\n        temp_data2 = pd.DataFrame()\n        results_pd_temp = results_pd[~((results_pd['test_flag']==1) & (results_pd['test_flag_agg']==0))]\n        for val in [1,0]:\n            temp_data = results_pd_temp[results_pd_temp['test_flag_agg']==val]\n            y_pred = temp_data['yhat']\n            y_true = temp_data['y']\n\n            temp_data1['test_flag_agg'] = val\n            # Eval. metrics calculation\n            temp_data1['mape'] = np.mean(np.abs(y_true - y_pred) / np.maximum(np.abs(y_true), epsilon))*100  \n            temp_data1['wmape'] = np.sum(np.abs(y_true - y_pred)) / np.maximum(np.sum(np.abs(y_true)),epsilon)*100  \n            temp_data1['bias'] = np.mean((y_true - y_pred))  \n            temp_data1['tracking_signal'] = np.sum((y_true - y_pred)) / np.mean(np.abs(y_true - y_pred))\n            temp_data1['mae'] = mean_absolute_error(y_true, y_pred)\n            temp_data1['rmse']=np.sqrt(mean_squared_error(y_true, y_pred))\n            temp_data2 = pd.concat([temp_data2,temp_data1],ignore_index = True)\n        \n        results_pd = pd.merge(results_pd,temp_data2,how='left',on='test_flag_agg')\n        \n        # To adhere to defined schema\n        for x in broadcast_gran:   \n            results_pd[x] = results_pd[x].astype(str)\n\n        # Append Hyperparameters used\n        for x in hp_config:\n            results_pd[x] = hpt[x].iloc[0]\n\n        if(len(temp_list)>0):\n            for var in temp_list:\n                results_pd[var] = 0\n      \n        results_pd.loc[results_pd['yhat']==0,['trend']+regressors+seasonal_cols+holidays_list] = 0\n        results_pd[\"window\"] = window_no\n\n        # Get the experiment id\n        tracking_value = broadcast_tracking.value.copy()\n            \n        if(mlflow_tracking_check.value == \"Out of Sample\" and tracking_value[\"tracking_needed\"] == True):\n\n            if(tracking_value['type']!=\"Managed\"):\n                if(tracking_value['tracking_uri'] is not None):\n                    mlflow.set_tracking_uri(\"file:\"+tracking_value['tracking_uri'])\n                    experiment_id = mlflow.set_experiment(tracking_value[\"mlflow_experiment_id\"])\n                    tracking_value['mlflow_experiment_id'] = experiment_id.experiment_id\n\n            #Add MLFlow code here\n            with mlflow.start_run(experiment_id = tracking_value['mlflow_experiment_id']):\n                mlflow.log_param('algorithm', 'Prophet')\n                mlflow.log_param('result_type', 'out_of_sample')\n                for x in broadcast_gran:\n                    mlflow.log_param(x, results_pd[x].iloc[0])\n                for x in hp_config:\n                    mlflow.log_param(x, results_pd[x].iloc[0])\n                test = results_pd[results_pd['test_flag']==1].reset_index(drop = True)\n                for x in [\"mape\",\"wmape\",\"bias\",\"tracking_signal\",\"mae\",\"rmse\"]:\n                    mlflow.log_metric(x, test[x].iloc[0])\n                    \n        results_pd['status'] = 'success'\n        return results_pd\n    except Exception as e:\n        if holidays_broadcast.value is not None:\n            holidays_list = list(holidays_broadcast.value['holiday'].unique()) + ['holidays']\n        else:\n            holidays_list = ['holidays']\n        results_pd = pd.DataFrame(columns = [['ds', 'y', 'yhat','yhat_upper','yhat_lower','mape','wmape','bias','tracking_signal','mae','rmse']+\\\n                                             list(broadcast_hyper_parameters.value.keys()) + ['status','test_flag','test_flag_agg','window'] + broadcast_granularity.value+\\\n                                             broadcast_regressors.value + ['trend','yearly','weekly','daily'] +\\\n                                            holidays_list],index = range(1))\n        results_pd[broadcast_granularity.value] = df_data[broadcast_granularity.value].head(1).reset_index(drop = True)\n        for x in broadcast_granularity.value:\n              results_pd[x] = results_pd[x].astype(str)\n        results_pd['status'] = str(e)\n        \n        return results_pd"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"82381fab-fab2-426e-836b-9ee459b21544","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Loading the latest Missing_value_treatment file\n##### Please update the reading path with the required data path if \"Missing value treatment\" was not run"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f4d8ee55-2d6f-4ee7-bf70-19b4242d30c2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Reading the latest input file based on timestamp\nall_files = [file for file in os.listdir(app_config['output_dir_path']+\"/Data_Processing/Missing_value_treatment\")]\nmissing_op_files = [file for file in all_files if \"Missing_value_treatment_results (\" in file]\nmissing_op_files = [file.replace(\".csv\",\"\") for file in missing_op_files]\nversion_dates = [datetime.strptime(x.split('(')[1].replace(')',''), '%Y-%m-%d-%H-%M-%S') for x in missing_op_files]\nmax_date = max(version_dates)\nmax_date = max_date.strftime('%Y-%m-%d-%H-%M-%S')\nreq_file_name = [x for x in missing_op_files if max_date in x]\nmissing_op_file_path = os.path.join(app_config['output_dir_path']+\"/Data_Processing/Missing_value_treatment\",req_file_name[0]+'.csv')\n# print(missing_op_file_path)\n\n# Reading the data\ndf = pd.read_csv(missing_op_file_path)\n# print(df.shape)\n\ndf.rename(columns = {ds_config:\"ds\", dv_config:\"y\"}, inplace = True)\ndf['ds'] = pd.to_datetime(df['ds'])\ndf[modeling_granularity_conf] = df[modeling_granularity_conf].astype(str)\n\nlogger.info(\"Data loaded\")\n# print(list(broadcast_hyper_parameters.value.keys()))\n\ngbcp = list(modeling_granularity_conf)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"af0d3cfe-71e6-4dbf-868f-ad26ecb3a865","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["if(app_config[\"validation\"][\"agg_metrics_req\"]):\n\n    # Creating windows and then calling the modeling function\n    test_periods = int(broadcast_test_periods.value)\n    window_test_periods = app_config[\"validation\"][\"agg_metrics_test_periods\"]\n    stride = app_config[\"validation\"][\"agg_metrics_stride\"]\n\n    # Getting the total number of weeks for each time series\n    temp_df = df.groupby(modeling_granularity_conf).agg({'ds':'count'}).rename(columns={'ds': '#total_weeks'}).reset_index()\n    df = df.merge(temp_df, on = modeling_granularity_conf ,how = \"left\")\n\n    unique_skuXds = df[modeling_granularity_conf+[\"#total_weeks\"]].drop_duplicates().reset_index(drop = True)\n\n    final_list = []\n    gran_len = len(modeling_granularity_conf)\n    \n    for row1 in range(0,len(unique_skuXds)): \n        Total_weeks = unique_skuXds.loc[row1,'#total_weeks']\n        train_interval = int(Total_weeks-test_periods)\n        j = 0\n        for train_i in range(train_interval,Total_weeks,stride):\n            if(train_i+window_test_periods <=Total_weeks):\n                test_i = train_i+window_test_periods\n                final_list.append([unique_skuXds.iloc[row1,index] for index in range(gran_len)] + [0,train_i,train_i+window_test_periods,j+1])\n                j += 1\n\n    # create all windows combination.\n    df_windows = pd.DataFrame([tuple(x) for x in final_list],columns =modeling_granularity_conf+['train_index_start','train_index_end','test_index_end','window_no'])\n    f_df = df.merge(df_windows,on=modeling_granularity_conf,how=\"left\")\n        \n    f_df['gran_tempp'] = f_df[gbcp+[\"window_no\"]].astype(str).sum(axis=1)\n    unique_pdts = f_df['gran_tempp'].unique()\n    new_results = pd.DataFrame()\n    for pdt in unique_pdts:\n        new_results = pd.concat([new_results,get_forecast_UDF(f_df[f_df['gran_tempp']==pdt])])\n            \n    new_results.to_csv(algo_path+\"/Out_of_sample_results_window_level (\"+datetime.today().strftime('%Y-%m-%d-%H-%M-%S')+\").csv\", index = False)\n    logger.info(\"Completed Backtesting\")\n    \n    # Reading the latest Out_of_sample_results_window_level file based on timestamp\n    all_files = [file for file in os.listdir(algo_path)]\n    backtesting_files = [file for file in all_files if \"Out_of_sample_results_window_level (\" in file]\n    backtesting_files = [file.replace(\".csv\",\"\") for file in backtesting_files]\n    version_dates = [datetime.strptime(x.split('(')[1].replace(')',''), '%Y-%m-%d-%H-%M-%S') for x in backtesting_files]\n    max_date = max(version_dates)\n    max_date = max_date.strftime('%Y-%m-%d-%H-%M-%S')\n    req_file_name = [x for x in backtesting_files if max_date in x]\n    backtesting_results_file_path = os.path.join(algo_path,req_file_name[0] + \".csv\")\n    print(backtesting_results_file_path)\n\n    # Reading the results of backtesting\n    df = pd.read_csv(backtesting_results_file_path)\n    df = df[df[\"status\"] == \"success\"]\n    \n    df[modeling_granularity_conf] = df[modeling_granularity_conf].astype(str)\n    df['ds'] = pd.to_datetime(df['ds'])\n\n    # Roll up the data at Modeling granularity window level\n    df_hyperparameters = best_hyperparam_results[gbcp + list(hyperparameters_conf)]\n\n    # performance metrics\n    per_met = ['status',\"test_flag_agg\",\"window\",\"mape\",\"wmape\",\"bias\",\"tracking_signal\",\"mae\",\"rmse\"]\n    df_metrics = df[gbcp + per_met].drop_duplicates()\n    df_metrics1 = df_metrics.groupby(gbcp + ['test_flag_agg','status'])[[\"mape\",\"wmape\",\"bias\",\"tracking_signal\",\"mae\",\"rmse\"]].mean().reset_index()\n\n    # Remaining columns\n    rem_cols = list(set(df.columns) - set(per_met+list(hyperparameters_conf))) + ['test_flag_agg']\n    dot_cols = [col for col in df.columns if \".\" in col] #to handle \".\"s\n    for col in dot_cols:\n        df.rename(columns = {col:col.replace(\".\",\"dot\")}, inplace = True)\n        rem_cols[rem_cols.index(col)] = col.replace(\".\",\"dot\")\n    rem_df = df[rem_cols]\n    \n    # Removing the training dates which falls in the test period\n    rem_df = rem_df[~((rem_df['test_flag']==1) & (rem_df['test_flag_agg']==0))]\n    group_cols = gbcp + ['ds','test_flag','test_flag_agg']\n    agg_cols = list(set(rem_cols) - set(group_cols))\n    exprs = {x: \"mean\" for x in agg_cols}\n    rem_df1 = rem_df.groupby(group_cols).agg(exprs).reset_index()\n    temp_cols = [col[:-1] if 'avg(' in col else col for col in rem_df1.columns ]\n    temp_cols = [col.replace('avg(','') for col in temp_cols]\n    rem_df1.columns = temp_cols\n\n    for col in dot_cols:\n        rem_df1.rename(columns = {col.replace(\".\",\"dot\"):col.replace(\"dot\",\".\")}, inplace = True)\n                            \n    # combining all the data\n    df_forecast = rem_df1.merge(df_metrics1, on = gbcp + ['test_flag_agg'], how='left')\n    df_forecast = df_forecast.merge(df_hyperparameters, on = gbcp , how='left')\n    \nelse:    \n    df['gran_tempp'] = df[gbcp].astype(str).sum(axis=1)\n    unique_pdts = df['gran_tempp'].unique()\n    df_forecast = pd.DataFrame()\n    for pdt in unique_pdts:\n        df_forecast = pd.concat([df_forecast,get_forecast_UDF(df[df['gran_tempp']==pdt])])\n            \ndel(df_forecast['test_flag_agg'])\ndf_forecast['algorithm'] = 'Prophet'"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d842c52c-f709-4763-8b16-f7f14b6ca835","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">11:15:06 - cmdstanpy - INFO - Chain [1] start processing\n11:15:06 - cmdstanpy - INFO - Chain [1] done processing\n11:15:11 - cmdstanpy - INFO - Chain [1] start processing\n11:15:11 - cmdstanpy - INFO - Chain [1] done processing\n11:15:18 - cmdstanpy - INFO - Chain [1] start processing\n11:15:18 - cmdstanpy - INFO - Chain [1] done processing\n11:15:23 - cmdstanpy - INFO - Chain [1] start processing\n11:15:23 - cmdstanpy - INFO - Chain [1] done processing\n11:15:29 - cmdstanpy - INFO - Chain [1] start processing\n11:15:29 - cmdstanpy - INFO - Chain [1] done processing\n11:15:34 - cmdstanpy - INFO - Chain [1] start processing\n11:15:34 - cmdstanpy - INFO - Chain [1] done processing\n11:15:40 - cmdstanpy - INFO - Chain [1] start processing\n11:15:43 - cmdstanpy - INFO - Chain [1] done processing\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">11:15:06 - cmdstanpy - INFO - Chain [1] start processing\n11:15:06 - cmdstanpy - INFO - Chain [1] done processing\n11:15:11 - cmdstanpy - INFO - Chain [1] start processing\n11:15:11 - cmdstanpy - INFO - Chain [1] done processing\n11:15:18 - cmdstanpy - INFO - Chain [1] start processing\n11:15:18 - cmdstanpy - INFO - Chain [1] done processing\n11:15:23 - cmdstanpy - INFO - Chain [1] start processing\n11:15:23 - cmdstanpy - INFO - Chain [1] done processing\n11:15:29 - cmdstanpy - INFO - Chain [1] start processing\n11:15:29 - cmdstanpy - INFO - Chain [1] done processing\n11:15:34 - cmdstanpy - INFO - Chain [1] start processing\n11:15:34 - cmdstanpy - INFO - Chain [1] done processing\n11:15:40 - cmdstanpy - INFO - Chain [1] start processing\n11:15:43 - cmdstanpy - INFO - Chain [1] done processing\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df_forecast.to_csv(algo_path+\"/Out_of_sample_evaluation_results (\"+datetime.today().strftime('%Y-%m-%d-%H-%M-%S')+\").csv\", index = False)\nlogger.info(\"Exported Out of sample evaluation results\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"165882d7-cef6-4389-914a-7a7e7e66c1b0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Predicting future timeperiods\nThe following code assumes that the X-variables for the required future time periods are available for each modeling granularity\n\nUncomment the below cells if wants to predict the future, update the df respectively such that it contains entire historical data as well as idvs data for the required future forecast time periods"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"db5df2c4-31d2-4272-b89a-b0bc7923bb13","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# broadcast_test_periods =  broadcast_variable_conf(4) # Provide the no. of timeperiods to forecast in the future"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"46b328c4-6202-4978-b65b-f2993e921189","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["## Reading the latest input file based on timestamp\n# all_files = [file for file in os.listdir(app_config['output_dir_path']+\"/Data_Processing/Missing_value_treatment\")]\n# missing_op_files = [file for file in all_files if \"Missing_value_treatment_results (\" in file]\n# missing_op_files = [file.replace(\".csv\",\"\") for file in missing_op_files]\n# version_dates = [datetime.strptime(x.split('(')[1].replace(')',''), '%Y-%m-%d-%H-%M-%S') for x in missing_op_files]\n# max_date = max(version_dates)\n# max_date = max_date.strftime('%Y-%m-%d-%H-%M-%S')\n# req_file_name = [x for x in missing_op_files if max_date in x]\n# missing_op_file_path = os.path.join(app_config['output_dir_path']+\"/Data_Processing/Missing_value_treatment\",req_file_name[0]+'.csv')\n## print(missing_op_file_path)\n\n## Reading the data\n# df = pd.read_csv(missing_op_file_path)\n## print(df.shape)\n\n# df.rename(columns = {ds_config:\"ds\", dv_config:\"y\"}, inplace = True)\n# df['ds'] = pd.to_datetime(df['ds'])\n# df[modeling_granularity_conf] = df[modeling_granularity_conf].astype(str)\n\n# # Broadcasting again with the \"Future forecast\" value since we won't be tracking the future forecast results\n# mlflow_tracking_check = broadcast_required_info(\"Future forecast\")\n# logger.info(\"Data which contains the future forecast periods is loaded\")\n\n# gbcp = list(modeling_granularity_conf)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9e28216a-81bf-4673-812d-2ab97b8d368b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# df['gran_tempp'] = df[gbcp].astype(str).sum(axis=1)\n# unique_pdts = df['gran_tempp'].unique()\n# df_forecast = pd.DataFrame()\n# for pdt in unique_pdts:\n#     df_forecast = pd.concat([df_forecast,get_forecast_UDF(df[df['gran_tempp']==pdt])])\n            \n# del(df_forecast['test_flag_agg'])\n# df_forecast['algorithm'] = 'Prophet'"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f2554632-bacf-4e9e-a096-3360d98b2e38","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# df_forecast.to_csv(algo_path + \"/Future_forecast_results (\"+datetime.today().strftime('%Y-%m-%d-%H-%M-%S')+\").csv\", index = False)\n# logger.info(\"Exported future forecast results\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"99ac5bcd-8a2a-4aaa-9145-0a87c506acf7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Exporting config file\nconfig_file_name = \"config_for_exp_id_\"+str(broadcast_tracking.value['mlflow_experiment_id']) + \" (\" +datetime.today().strftime('%Y-%m-%d-%H-%M-%S-%f')[:-3]+\").yml\"\nconfig_path1 = os.path.join(config_path,config_file_name)\nwith open(config_path1, 'w') as file:\n    yaml.dump(temp_config, file, default_flow_style=False,sort_keys=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"39f455b0-9805-4951-9c4f-a8017f389b7f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Move from tmp directory to req. location in datalake\nimport platform\nplat_sys = platform.system()\n\nif(plat_sys!='Windows'):\n    log_file = log_file.replace(' (', '\\ \\(').replace(')','\\)')\n    os.system('mv /tmp/{0} {1}'.format(log_file,logs_path))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bc65feb5-77a5-4803-929f-e71d0cfbdf37","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"1.2 - Model evaluation, Retraining & Scoring","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":98246570296228}},"nbformat":4,"nbformat_minor":0}