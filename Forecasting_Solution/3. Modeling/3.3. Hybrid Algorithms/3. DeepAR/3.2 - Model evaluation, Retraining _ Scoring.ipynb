{"cells":[{"cell_type":"markdown","source":["### Objective:\nThe objective of the notebook is to -\n* Build the final model of ElasticNet algorithm using the best hyperparameter set (identified using Backtesting) and score the test set to get a performance metric\n* For forecasting future periods, we will re-train the model with the same hyperparameter set on the train + validation + test set to capture the patterns in the test set and then forecast future N periods"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3244205d-09ab-4dd6-9930-3abc03cd8091","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import yaml\nimport inspect\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom distutils.command.config import config\nfrom tqdm.auto import tqdm\nfrom datetime import timedelta\nfrom datetime import datetime\nimport mlflow\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nimport os\nimport dotsi\nimport logging\n\nimport mxnet as mx\nfrom gluonts.dataset.pandas import PandasDataset\nfrom gluonts.mx import DeepAREstimator\nfrom gluonts.mx import Trainer\nfrom gluonts.evaluation import make_evaluation_predictions, Evaluator"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8afbc8bf-179f-4ffb-893e-37b23843914e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# logging part\np_dir = \"/tmp/\"\nlog_file = \"DeepAR_model_eval_retraining_scoring\" + \" (\" +datetime.today().strftime('%Y-%m-%d-%H-%M-%S')+ \").log\"\n\nlogger = logging.getLogger('custom_log')\nlogger.setLevel(logging.DEBUG)\n\n# Applying necessary formatter\nfh = logging.FileHandler(p_dir+log_file)\nformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\nfh.setFormatter(formatter)\nlogger.addHandler(fh)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6894dc1d-f97c-45c7-a5dd-3ec4ee2964db","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Getting the default settings of hyperparameters. Used to check that user-provided hyperparameters must always be a subset of these.\ndef get_default_args(func) -> dict:\n    \"\"\"Function to get the default values of the hyperparameters for the given algorithm\n\n    Parameters\n    ----------\n    func : constructor of the respective algorithm\n        The name of the algorithm (Eg: Prophet,SARIMAX)\n\n    Returns\n    -------\n    dict\n        returns a dictionary of hyperparameters and the corresponding default values for the given algorithm\n    \"\"\"\n    signature = inspect.signature(func)\n    return {\n        k: v.default if v.default is not inspect.Parameter.empty else None\n        for k, v in signature.parameters.items()\n        if k != 'self'\n    }\n    \ndefault_hpps_init = get_default_args(DeepAREstimator)\ndefault_hpps_fit = get_default_args(Trainer)\n\nif('trainer' in default_hpps_init.keys()):\n    del default_hpps_init['trainer']"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"497f0fb9-2e94-40d4-9fea-50103670624e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%run ../../../0_Config.ipynb"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b3b174e4-abbc-45b5-8235-7376db53681a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["logger.info(\"Config file read\")\nassert set(app_config[\"Algorithms\"][\"DeepAR\"][\"Hyperparameters\"]['Estimator'].keys()).\\\n           issubset(set(default_hpps_init.keys())),\\\n           'keys supplied by the user for the DeepAR Algorithm under __init__ method must be valid'\nassert set(app_config[\"Algorithms\"][\"DeepAR\"][\"Hyperparameters\"]['Trainer'].keys()).\\\n           issubset(set(default_hpps_fit.keys())),\\\n           'keys supplied by the user for the DeepAR Algorithm under fit method must be valid'\n\n# For exporting the config file\ntemp_config = app_config.copy()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4a08f9d9-4c5c-4e70-a595-bee153da17f8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">INFO:custom_log:Config file read\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">INFO:custom_log:Config file read\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["def frange(start,stop,step= 1):\n    l = []\n    i = start\n    while(i < stop):\n        l.append(round(i,len(str(step))))\n        i = i+step\n    return l\n\ndef drange(hyperparameters):\n    l=[]\n    for key in hyperparameters.keys():\n        val = hyperparameters[key]\n        if 'range' in val:\n            val = val.replace('range','frange')\n            new_str = 'total_list = '  + val\n            _locals = locals()\n            exec(new_str,globals(),_locals)\n            without_dup = list(set(_locals['total_list']))\n            hyperparameters[key] = without_dup\n    return hyperparameters"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b5e50f7c-ea47-4d62-8f43-28f4d6f224f8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["fit_ = drange(app_config['Algorithms']['DeepAR']['Hyperparameters']['Trainer'])\ninit_ = drange(app_config['Algorithms']['DeepAR']['Hyperparameters']['Estimator'])\nfor key in init_.keys():\n    if(key in fit_.keys()):\n        fit_[key] = list(set(fit_[key]+init_[key]))\n    else:\n        fit_[key] = list(init_[key])\n    \nfit_new = {}\nfor key in fit_.keys():\n    temp = []\n    for val in fit_[key]:\n        if(type(val) == list):\n            val = str(val)\n        if((val!='None') and (val!='Null') and (val!=None)):\n            temp.append(val)\n    if(len(temp)>0):\n        fit_new[key] = temp\n    \nfor val in ['prediction_length','freq','use_feat_static_real','use_feat_static_cat','cardinality']:\n    if(val in fit_new.keys()):\n        del fit_new[val]\n    \nif(len(app_config['Algorithms']['DeepAR']['exogenous_variables']['feat_dynamic_real'])==0):\n    fit_new['use_feat_dynamic_real'] = [False]\nelse:\n    fit_new['use_feat_dynamic_real'] = [True]\n    \napp_config['Algorithms']['DeepAR']['Hyperparameters'] = fit_new"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0fadf3ea-771b-48cb-a5d2-835fa2838158","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Create the algo directory for storing the results\noutput_directory = app_config['output_dir_path']\nroot_dir = \"Modeling_Results\"\nalgorithm = \"DeepAR\"\nalgo_path = os.path.join(output_directory,root_dir,algorithm)\nif not os.path.exists(algo_path):\n    os.makedirs(algo_path)\nlogger.info(\"Created algorithm directory\")    \n\nlogs_path = os.path.join(output_directory,root_dir,'logs',algorithm)\nif not os.path.exists(logs_path):\n    os.makedirs(logs_path)\nlogger.info(\"Created logs directory\")\n\nconfig_path = os.path.join(app_config['output_dir_path'],\"Modeling_Results\",\"config\")\nif not os.path.exists(config_path):\n    os.makedirs(config_path)\nlogger.info(\"Created config directory\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bdb403d4-f130-4e55-86d6-c0717353e44f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">INFO:custom_log:Created algorithm directory\nINFO:custom_log:Created logs directory\nINFO:custom_log:Created config directory\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">INFO:custom_log:Created algorithm directory\nINFO:custom_log:Created logs directory\nINFO:custom_log:Created config directory\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["hyperparameters_conf = dict(app_config[\"Algorithms\"][\"DeepAR\"][\"Hyperparameters\"])\n# print(hyperparameters_conf)\n\nmodeling_granularity_conf = app_config[\"modeling_granularity\"]\nglobal_modeling_granularity_conf = app_config[\"Algorithms\"][\"DeepAR\"][\"global_model_gran\"]\n# print(modeling_granularity_conf)\n\n# sales variable broadcast\nif(app_config[\"Algorithms\"][\"DeepAR\"][\"sales_amt_variable\"]==app_config[\"dependent_variable\"]):\n    app_config[\"Algorithms\"][\"DeepAR\"][\"sales_amt_variable\"] = 'y'\nbroadcast_sales = dotsi.Dict({\"value\":app_config[\"Algorithms\"][\"DeepAR\"][\"sales_amt_variable\"]})\n\n# Rename Start date and DV config\ndv_config = app_config[\"dependent_variable\"]\nds_config = app_config[\"date_var\"]\n\n# pos and neg corr broadcast\ncorr_config = dict(app_config['Algorithms']['DeepAR']['exogenous_variables'])\ncorr_config_broadcast = dotsi.Dict({\"value\":corr_config})\n\n# Eval metric broadcast\nbroadcast_metric = dotsi.Dict({\"value\":app_config['validation']['metric']})\nbroadcast_test_periods = dotsi.Dict({\"value\":app_config[\"validation\"][\"no_of_test_periods\"]})\nbroadcast_agg_test_periods = dotsi.Dict({\"value\":app_config[\"validation\"][\"agg_metrics_test_periods\"]})\n\nbroadcast_granularity = dotsi.Dict({\"value\":modeling_granularity_conf})\nglobal_broadcast_granularity = dotsi.Dict({\"value\":global_modeling_granularity_conf})\nbroadcast_hyper_parameters = dotsi.Dict({\"value\":hyperparameters_conf})\nbroadcast_agg_metrics_req = dotsi.Dict({\"value\":app_config[\"validation\"][\"agg_metrics_req\"]})\nbroadcast_tracking = dotsi.Dict({\"value\":app_config['tracking']})\nmlflow_tracking_check = dotsi.Dict({\"value\":\"Out of Sample\"})\nlogger.info(\"Broadcasted the required variables\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a06d2154-c076-49d8-bb35-d7a3809fd115","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">INFO:custom_log:Broadcasted the required variables\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">INFO:custom_log:Broadcasted the required variables\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Reading the latest file based on timestamp\nall_files = [file for file in os.listdir(algo_path)]\nbest_hyp_files = [file for file in all_files if \"Best_hyperparameters (\" in file]\nbest_hyp_files = [file.replace(\".csv\",\"\") for file in best_hyp_files]\nversion_dates = [datetime.strptime(x.split('(')[1].replace(')',''), '%Y-%m-%d-%H-%M-%S') for x in best_hyp_files]\nmax_date = max(version_dates)\nmax_date = max_date.strftime('%Y-%m-%d-%H-%M-%S')\nreq_file_name = [x for x in best_hyp_files if max_date in x]\nbest_hyp_param_results_file_path = os.path.join(algo_path,req_file_name[0]+\".csv\")\nprint(best_hyp_param_results_file_path)\n\nbest_hyperparam_results = pd.read_csv(best_hyp_param_results_file_path)\nbest_hyperparam_results = best_hyperparam_results[best_hyperparam_results['status']=='success'].reset_index(drop = True)\nif(len(corr_config['feat_dynamic_real'])==0):\n    best_hyperparam_results['use_feat_dynamic_real'] = False\nelse:\n    best_hyperparam_results['use_feat_dynamic_real'] = True\n    \nbest_hyperparam_results[global_modeling_granularity_conf] = best_hyperparam_results[global_modeling_granularity_conf].astype(str)\nbest_hyperparam_results.replace(['true'],True, inplace = True)\nbest_hyperparam_results.replace(['false'],False, inplace = True)\nbest_hyperparam_results_broadcast = dotsi.Dict({\"value\":best_hyperparam_results})\nlogger.info(\"Read the best hyperparamter results\")\nbest_hyperparam_results"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b0058222-f420-49c7-a6ed-6593783f8270","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/dbfs/mnt/solutionsadls_data/modeling_results_python/Modeling_Results/DeepAR/Best_hyperparameters (2023-01-13-08-38-01).csv\nINFO:custom_log:Read the best hyperparamter results\nOut[117]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/dbfs/mnt/solutionsadls_data/modeling_results_python/Modeling_Results/DeepAR/Best_hyperparameters (2023-01-13-08-38-01).csv\nINFO:custom_log:Read the best hyperparamter results\nOut[117]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Div_No</th>\n      <th>epochs</th>\n      <th>weight_decay</th>\n      <th>num_cells</th>\n      <th>num_layers</th>\n      <th>dropout_rate</th>\n      <th>use_feat_dynamic_real</th>\n      <th>mape</th>\n      <th>wmape</th>\n      <th>bias</th>\n      <th>tracking_signal</th>\n      <th>mae</th>\n      <th>rmse</th>\n      <th>status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24</td>\n      <td>1</td>\n      <td>1.000000e-08</td>\n      <td>30</td>\n      <td>5</td>\n      <td>0.06</td>\n      <td>True</td>\n      <td>157.990576</td>\n      <td>83.008765</td>\n      <td>5.254875</td>\n      <td>1.539248</td>\n      <td>10.525528</td>\n      <td>12.497816</td>\n      <td>success</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Div_No</th>\n      <th>epochs</th>\n      <th>weight_decay</th>\n      <th>num_cells</th>\n      <th>num_layers</th>\n      <th>dropout_rate</th>\n      <th>use_feat_dynamic_real</th>\n      <th>mape</th>\n      <th>wmape</th>\n      <th>bias</th>\n      <th>tracking_signal</th>\n      <th>mae</th>\n      <th>rmse</th>\n      <th>status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24</td>\n      <td>1</td>\n      <td>1.000000e-08</td>\n      <td>30</td>\n      <td>5</td>\n      <td>0.06</td>\n      <td>True</td>\n      <td>157.990576</td>\n      <td>83.008765</td>\n      <td>5.254875</td>\n      <td>1.539248</td>\n      <td>10.525528</td>\n      <td>12.497816</td>\n      <td>success</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_forecast_UDF(df_data: pd.DataFrame)-> pd.DataFrame:\n    \"\"\"Function to perform final model building using the train data and score on the test data utilizing the broadcasted details from the config file\n\n    Parameters\n    ----------\n    df_data : pd.DataFrame\n        The dataset containing values for all the required variables\n\n    Returns\n    -------\n    pd.DataFrame\n        Returns a dataframe with the granularity,date,independent variables contributions if any and performance metrics for the training and the testing set\n    \"\"\"\n    try:\n        \n        # broadcast_granularity\n        broadcast_gran = broadcast_granularity.value\n        global_broadcast_gran = global_broadcast_granularity.value\n        \n        hpt = best_hyperparam_results_broadcast.value\n        # get best hyperparameters for the given modeling granularity\n        for x in list(global_broadcast_gran):\n            hpt = hpt[hpt[x] == df_data[x].iloc[0]]\n            \n        df_data['ts_identifier'] = df_data[broadcast_gran].astype(str).apply(\"_\".join, axis=1)\n        df_data = df_data.sort_values(by=['ts_identifier','ds'],ascending = True)\n        df_data.index = df_data['ds']\n        \n        corr_var = corr_config_broadcast.value\n        regressors = list(set(corr_var[\"feat_dynamic_real\"]))\n\n        train = pd.DataFrame()\n        test = pd.DataFrame()\n        ts_comb = df_data['ts_identifier'].unique()\n        if(broadcast_agg_metrics_req.value == True): \n            prediction_length = broadcast_agg_test_periods.value\n            window_no = str(df_data[\"window_no\"].iloc[0])\n            for comb in ts_comb:\n                temp_data = df_data[df_data['ts_identifier']==comb]\n                train_index_end = temp_data[\"train_index_end\"].iloc[0]\n                test_i = temp_data[\"test_index_end\"].iloc[0]\n                train = pd.concat([train,temp_data.iloc[:train_index_end]],ignore_index = False)\n                test = pd.concat([test,temp_data.iloc[:test_i]],ignore_index = False)\n        else:\n            prediction_length = broadcast_test_periods.value\n            for comb in ts_comb:\n                temp_data = df_data[df_data['ts_identifier']==comb]\n                train = pd.concat([train,temp_data.iloc[:-prediction_length]],ignore_index = False)\n            test = df_data.copy()\n            window_no = str(1)\n            \n        train_dataset = PandasDataset.from_long_dataframe(\n            train,\n            item_id=\"ts_identifier\",\n            feat_dynamic_real=regressors,\n            target = 'y'\n        )\n        test_dataset = PandasDataset.from_long_dataframe(\n            test,\n            item_id=\"ts_identifier\",\n            feat_dynamic_real=regressors,\n            target = 'y'\n        )\n        freq = train_dataset.freq\n\n        # Updating the default arguments with the parameters provided in the config\n        hp_config = broadcast_hyper_parameters.value\n\n        def_args = get_default_args(Trainer)\n        for x in hp_config:\n            if(x in def_args.keys()):\n                temp_val = hpt[x].iloc[0]\n                if(type(temp_val)==str):\n                    if('[' in temp_val):\n                        temp_val = eval(temp_val)\n                def_args[x] = temp_val\n\n        def_args_init = get_default_args(DeepAREstimator)\n        for x in hp_config:\n            if(x in def_args_init.keys()):\n                temp_val = hpt[x].iloc[0]\n                if(type(temp_val)==str):\n                    if('[' in temp_val):\n                        temp_val = eval(temp_val)\n                def_args_init[x] = temp_val\n\n        def_args_init['trainer'] = Trainer(**def_args)\n        def_args_init['prediction_length'] = prediction_length \n        def_args_init['freq'] = freq\n\n        estimator = DeepAREstimator(**def_args_init)\n\n        mx.random.seed(7)\n        np.random.seed(7)\n        predictor = estimator.train(train_dataset)\n\n        forecast_it,_ = make_evaluation_predictions(dataset=test_dataset, predictor=predictor)\n        forecasts = list(forecast_it)\n        reslts_df_list=[]\n\n        for ts_forecasts in forecasts:\n            df_temp= pd.DataFrame({\"ds\" : pd.date_range(ts_forecasts.start_date.to_timestamp(),\\\n                                                                 periods=prediction_length, freq=freq),\n                                'yhat' : ts_forecasts.samples.mean(axis=0),\n                                'ts_identifier' : ts_forecasts.item_id},\n                                )\n            reslts_df_list.append(df_temp)\n\n        df_predictions = pd.concat(reslts_df_list)\n\n        # Sales or Quantity can't be negative hence\n        df_predictions[\"yhat\"] = np.where(df_predictions[\"yhat\"]<0,0,df_predictions[\"yhat\"])\n        gran = list(set(broadcast_gran+global_broadcast_gran))\n        results_pd = df_data[gran+['ts_identifier','ds','y']].reset_index(drop = True)\n        results_pd=pd.merge(results_pd,df_predictions,on=['ts_identifier','ds'],how='right')\n        results_pd['test_flag'] = 1\n                              \n        # Eval. metrics calculation\n        # to handle erroneous results epsilon is set to 1.\n        epsilon = 1\n        temp_data1 = pd.DataFrame(index= range(1))\n        temp_data2 = pd.DataFrame()\n        for comb in ts_comb:\n            temp_data = results_pd[results_pd['ts_identifier']==comb]\n            y_pred = temp_data['yhat']\n            y_true = temp_data['y']\n            temp_data1['ts_identifier'] = comb\n            # Eval. metrics calculation\n            temp_data1['mape'] = np.mean(np.abs(y_true - y_pred) / np.maximum(np.abs(y_true), epsilon))*100  \n            temp_data1['wmape'] = np.sum(np.abs(y_true - y_pred)) / np.maximum(np.sum(np.abs(y_true)),epsilon)*100  \n            temp_data1['bias'] = np.mean((y_true - y_pred))  \n            temp_data1['tracking_signal'] = np.sum((y_true - y_pred)) / np.mean(np.abs(y_true - y_pred))\n            temp_data1['mae'] = mean_absolute_error(y_true, y_pred)\n            temp_data1['rmse']=np.sqrt(mean_squared_error(y_true, y_pred))\n            temp_data2 = pd.concat([temp_data2,temp_data1],ignore_index = True)\n\n        results_pd = pd.merge(results_pd,temp_data2,how='left',on='ts_identifier')\n        \n        # To adhere to defined schema\n        for x in gran:   \n            results_pd[x] = results_pd[x].astype(str)\n\n        # Append Hyperparameters used\n        for x in hp_config:\n            results_pd[x] = hpt[x].iloc[0]\n        \n        del(results_pd['ts_identifier'])\n        # Get the experiment id\n        tracking_value = broadcast_tracking.value.copy()\n        if(mlflow_tracking_check.value == \"Out of Sample\" and tracking_value[\"tracking_needed\"] == True):\n            if(tracking_value['type']!=\"Managed\"):\n                if(tracking_value['tracking_uri'] is not None):\n                    mlflow.set_tracking_uri(\"file:\"+tracking_value['tracking_uri'])\n                    experiment_id = mlflow.set_experiment(tracking_value[\"mlflow_experiment_id\"])\n                    tracking_value['mlflow_experiment_id'] = experiment_id.experiment_id\n            #Add MLFlow code here\n            with mlflow.start_run(experiment_id = tracking_value['mlflow_experiment_id']):\n                mlflow.log_param('algorithm', 'DeepAR')\n                mlflow.log_param('result_type', 'out_of_sample')\n                for x in global_broadcast_gran:\n                    mlflow.log_param(x, results_pd[x].iloc[0])\n                for x in hp_config:\n                    mlflow.log_param(x, results_pd[x].iloc[0])\n                temp_test = results_pd[results_pd['test_flag']==1].reset_index(drop = True)\n                for x in [\"mape\",\"wmape\",\"bias\",\"tracking_signal\",\"mae\",\"rmse\"]:\n                    mlflow.log_metric(x, temp_test[x].iloc[0])\n                              \n        results_pd[\"window\"] = window_no\n        results_pd['status'] = 'success'\n        return results_pd\n    \n    except Exception as e:\n        gran = list(set(broadcast_granularity.value+global_broadcast_granularity.value))\n        results_pd = pd.DataFrame(columns = [['ds', 'y', 'yhat','mape','wmape','bias',\\\n                                            'tracking_signal','mae','rmse']+\\\n                        list(broadcast_hyper_parameters.value.keys()) + ['status','test_flag',\"window\"] + gran],index = range(1))\n        results_pd[gran] = df_data[gran].head(1).reset_index(drop = True)\n        for x in gran:\n            results_pd[x] = results_pd[x].astype(str)\n        results_pd['status'] = str(e)\n        return results_pd"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"220bf9c7-35b6-4241-a52a-6c4e8324f0b6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Loading the latest Missing_value_treatment file\n##### Please update the reading path with the required data path if \"Missing value treatment\" was not run"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a3e2fc8f-d3ab-428c-a4a9-70f2e9b44549","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Reading the latest input file based on timestamp\nall_files = [file for file in os.listdir(app_config['output_dir_path']+\"/Data_Processing/Missing_value_treatment\")]\nmissing_op_files = [file for file in all_files if \"Missing_value_treatment_results (\" in file]\nmissing_op_files = [file.replace(\".csv\",\"\") for file in missing_op_files]\nversion_dates = [datetime.strptime(x.split('(')[1].replace(')',''), '%Y-%m-%d-%H-%M-%S') for x in missing_op_files]\nmax_date = max(version_dates)\nmax_date = max_date.strftime('%Y-%m-%d-%H-%M-%S')\nreq_file_name = [x for x in missing_op_files if max_date in x]\nmissing_op_file_path = os.path.join(app_config['output_dir_path']+\"/Data_Processing/Missing_value_treatment\",req_file_name[0] + \".csv\")\n# print(missing_op_file_path)\n\n# Reading the data\ndf = pd.read_csv(missing_op_file_path)\n# print(df.shape)\n\ndf.rename(columns = {ds_config:\"ds\", dv_config:\"y\"}, inplace = True)\ndf['ds'] = pd.to_datetime(df['ds'])\ndf[list(set(modeling_granularity_conf+global_modeling_granularity_conf))] = df[list(set(modeling_granularity_conf+global_modeling_granularity_conf))].astype(str)\n\nlogger.info(\"Data loaded\")\n# print(list(broadcast_hyper_parameters.value.keys()))\n\ngbcp = list(global_modeling_granularity_conf)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"af0d3cfe-71e6-4dbf-868f-ad26ecb3a865","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">INFO:custom_log:Data loaded\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">INFO:custom_log:Data loaded\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[" if(app_config[\"validation\"][\"agg_metrics_req\"]):\n\n    # Creating windows and then calling the modeling function\n    test_periods = int(broadcast_test_periods.value)\n    window_test_periods = app_config[\"validation\"][\"agg_metrics_test_periods\"]\n    stride = app_config[\"validation\"][\"agg_metrics_stride\"]\n\n    # Getting the total number of weeks for each time series\n    temp_df = df.groupby(modeling_granularity_conf).agg({'ds':'count'}).rename(columns={'ds': '#total_weeks'}).reset_index()\n    df = df.merge(temp_df, on = modeling_granularity_conf ,how = \"left\")\n\n    unique_skuXds = df[modeling_granularity_conf+[\"#total_weeks\"]].drop_duplicates().reset_index(drop = True)\n\n    final_list = []\n    gran_len = len(modeling_granularity_conf)\n    \n    for row1 in range(0,len(unique_skuXds)): \n        Total_weeks = unique_skuXds.loc[row1,'#total_weeks']\n        train_interval = int(Total_weeks-test_periods)\n        j = 0\n        for train_i in range(train_interval,Total_weeks,stride):\n            if(train_i+window_test_periods <=Total_weeks):\n                test_i = train_i+window_test_periods\n                final_list.append([unique_skuXds.iloc[row1,index] for index in range(gran_len)] + [0,train_i,train_i+window_test_periods,j+1])\n                j += 1\n\n    # create all windows combination.\n    df_windows = pd.DataFrame([tuple(x) for x in final_list],columns =modeling_granularity_conf+['train_index_start','train_index_end','test_index_end','window_no'])\n    f_df = df.merge(df_windows,on=modeling_granularity_conf,how=\"left\")\n        \n    f_df['gran_tempp'] = f_df[gbcp+[\"window_no\"]].astype(str).sum(axis=1)\n    unique_pdts = f_df['gran_tempp'].unique()\n    new_results = pd.DataFrame()\n    for pdt in unique_pdts:\n        new_results = pd.concat([new_results,get_forecast_UDF(f_df[f_df['gran_tempp']==pdt])])\n            \n    new_results.to_csv(algo_path+\"/Out_of_sample_results_window_level (\"+datetime.today().strftime('%Y-%m-%d-%H-%M-%S')+\").csv\", index = False)\n    logger.info(\"Completed Backtesting\")\n    \n    # Reading the latest Out_of_sample_results_window_level file based on timestamp\n    all_files = [file for file in os.listdir(algo_path)]\n    backtesting_files = [file for file in all_files if \"Out_of_sample_results_window_level (\" in file]\n    backtesting_files = [file.replace(\".csv\",\"\") for file in backtesting_files]\n    version_dates = [datetime.strptime(x.split('(')[1].replace(')',''), '%Y-%m-%d-%H-%M-%S') for x in backtesting_files]\n    max_date = max(version_dates)\n    max_date = max_date.strftime('%Y-%m-%d-%H-%M-%S')\n    req_file_name = [x for x in backtesting_files if max_date in x]\n    backtesting_results_file_path = os.path.join(algo_path,req_file_name[0] + \".csv\")\n    print(backtesting_results_file_path)\n\n    # Reading the results of backtesting\n    df = pd.read_csv(backtesting_results_file_path)\n    df = df[df[\"status\"] == \"success\"]\n    \n    gran = list(set(gbcp +modeling_granularity_conf))\n    df[gran] = df[gran].astype(str)\n    df['ds'] = pd.to_datetime(df['ds'])\n    \n    # Roll up the data at Modeling granularity window level\n    df_hyperparameters = best_hyperparam_results[gbcp + list(hyperparameters_conf)]\n\n    # performance metrics\n    per_met = ['status',\"test_flag\",\"window\",\"mape\",\"wmape\",\"bias\",\"tracking_signal\",\"mae\",\"rmse\"]\n    df_metrics = df[gran + per_met].drop_duplicates()\n    df_metrics1 = df_metrics.groupby(gran + ['test_flag','status'])[[\"mape\",\"wmape\",\"bias\",\"tracking_signal\",\"mae\",\"rmse\"]].mean().reset_index()\n\n    # Remaining columns\n    rem_cols = list(set(df.columns) - set(per_met+list(hyperparameters_conf))) + ['test_flag']\n    dot_cols = [col for col in df.columns if \".\" in col] #to handle \".\"s\n    for col in dot_cols:\n        df.rename(columns = {col:col.replace(\".\",\"dot\")}, inplace = True)\n        rem_cols[rem_cols.index(col)] = col.replace(\".\",\"dot\")\n    rem_df = df[rem_cols]\n        \n    group_cols = gran + ['ds','test_flag']\n    agg_cols = list(set(rem_cols) - set(group_cols))\n    exprs = {x: \"mean\" for x in agg_cols}\n    rem_df1 = rem_df.groupby(group_cols).agg(exprs).reset_index()\n    temp_cols = [col[:-1] if 'avg(' in col else col for col in rem_df1.columns ]\n    temp_cols = [col.replace('avg(','') for col in temp_cols]\n    rem_df1.columns = temp_cols\n\n    for col in dot_cols:\n        rem_df1.rename(columns = {col.replace(\".\",\"dot\"):col.replace(\"dot\",\".\")}, inplace = True)\n        \n    # combining all the data\n    df_forecast = rem_df1.merge(df_metrics1, on = gran + ['test_flag'], how='left')\n    df_forecast = df_forecast.merge(df_hyperparameters, on = gbcp , how='left')\n    \nelse:    \n    df['gran_tempp'] = df[gbcp].astype(str).sum(axis=1)\n    unique_pdts = df['gran_tempp'].unique()\n    df_forecast = pd.DataFrame()\n    for pdt in unique_pdts:\n        df_forecast = pd.concat([df_forecast,get_forecast_UDF(df[df['gran_tempp']==pdt])])\n            \ndf_forecast['algorithm'] = 'DeepAR'"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7cb61114-bf9b-4f36-bfb8-7d8783715c08","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">\r  0%|          | 0/50 [00:00&lt;?, ?it/s]\r100%|██████████| 50/50 [00:06&lt;00:00,  7.24it/s, epoch=1/1, avg_epoch_loss=3.6]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\r  0%|          | 0/50 [00:00&lt;?, ?it/s]\r100%|██████████| 50/50 [00:06&lt;00:00,  7.24it/s, epoch=1/1, avg_epoch_loss=3.6]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df_forecast.to_csv(algo_path+\"/Out_of_sample_evaluation_results (\"+datetime.today().strftime('%Y-%m-%d-%H-%M-%S')+\").csv\", index = False)\nlogger.info(\"Exported Out of sample evaluation results\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"165882d7-cef6-4389-914a-7a7e7e66c1b0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">INFO:custom_log:Exported Out of sample evaluation results\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">INFO:custom_log:Exported Out of sample evaluation results\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Predicting future timeperiods\nThe following code assumes that the X-variables for the required future time periods are available for each modeling granularity\n\nUncomment the below cells if wants to predict the future, update the df respectively such that it contains entire historical data as well as idvs data for the required future forecast time periods"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"db5df2c4-31d2-4272-b89a-b0bc7923bb13","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# broadcast_test_periods =  broadcast_variable_conf(4) # Provide the no. of timeperiods to forecast in the future"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"46b328c4-6202-4978-b65b-f2993e921189","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["## Reading the latest input file based on timestamp\n# all_files = [file for file in os.listdir(app_config['output_dir_path']+\"/Data_Processing/Missing_value_treatment\")]\n# missing_op_files = [file for file in all_files if \"Missing_value_treatment_results (\" in file]\n# missing_op_files = [file.replace(\".csv\",\"\") for file in missing_op_files]\n# version_dates = [datetime.strptime(x.split('(')[1].replace(')',''), '%Y-%m-%d-%H-%M-%S') for x in missing_op_files]\n# max_date = max(version_dates)\n# max_date = max_date.strftime('%Y-%m-%d-%H-%M-%S')\n# req_file_name = [x for x in missing_op_files if max_date in x]\n# missing_op_file_path = os.path.join(app_config['output_dir_path']+\"/Data_Processing/Missing_value_treatment\",req_file_name[0]+'.csv')\n## print(missing_op_file_path)\n\n## Reading the data\n# df = pd.read_csv(missing_op_file_path)\n## print(df.shape)\n\n# df.rename(columns = {ds_config:\"ds\", dv_config:\"y\"}, inplace = True)\n# df['ds'] = pd.to_datetime(df['ds'])\n# df[modeling_granularity_conf] = df[modeling_granularity_conf].astype(str)\n\n# # Broadcasting again with the \"Future forecast\" value since we won't be tracking the future forecast results\n# mlflow_tracking_check = broadcast_required_info(\"Future forecast\")\n# logger.info(\"Data which contains the future forecast periods is loaded\")\n\n# gbcp = list(global_modeling_granularity_conf)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7929ea30-31e4-4425-a9bd-c66066a4b863","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# df['gran_tempp'] = df[gbcp].astype(str).sum(axis=1)\n# unique_pdts = df['gran_tempp'].unique()\n# df_forecast = pd.DataFrame()\n# for pdt in unique_pdts:\n#     df_forecast = pd.concat([df_forecast,get_forecast_UDF(df[df['gran_tempp']==pdt])])\n            \n# del(df_forecast['test_flag_agg'])\n# df_forecast['algorithm'] = 'DeepAR'"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"75c743bf-068c-476a-9040-f7055e43669e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# df_forecast.to_csv(algo_path + \"/Future_forecast_results (\"+datetime.today().strftime('%Y-%m-%d-%H-%M-%S')+\").csv\", index = False)\n# logger.info(\"Exported future forecast results\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"99ac5bcd-8a2a-4aaa-9145-0a87c506acf7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Exporting config file\nconfig_file_name = \"config_for_exp_id_\"+str(broadcast_tracking.value['mlflow_experiment_id']) + \" (\" +datetime.today().strftime('%Y-%m-%d-%H-%M-%S-%f')[:-3]+\").yml\"\nconfig_path1 = os.path.join(config_path,config_file_name)\nwith open(config_path1, 'w') as file:\n    yaml.dump(temp_config, file, default_flow_style=False,sort_keys=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f3f88afa-3bd3-4ba5-b628-a4f247fad3ae","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Move from tmp directory to req. location in datalake\nimport platform\nplat_sys = platform.system()\n\nif(plat_sys!='Windows'):\n    log_file = log_file.replace(' (', '\\ \\(').replace(')','\\)')\n    os.system('mv /tmp/{0} {1}'.format(log_file,logs_path))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b1f6b147-5c7b-48e5-9524-dc5218a52297","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"3.2 - Model evaluation, Retraining & Scoring","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":98246570296195}},"nbformat":4,"nbformat_minor":0}